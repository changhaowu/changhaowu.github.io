I"»<ul class="table-of-content" id="markdown-toc">
  <li><a href="#generative-model-part-1a-survey-on-variational-autoencoders" id="markdown-toc-generative-model-part-1a-survey-on-variational-autoencoders">Generative Model Part 1ï¼šA Survey on Variational Autoencoders</a>    <ul>
      <li><a href="#1--introduction" id="markdown-toc-1--introduction">1.  Introduction</a>        <ul>
          <li><a href="#11--probabilistic-models-and-variational-inference" id="markdown-toc-11--probabilistic-models-and-variational-inference">1.1  Probabilistic Models and Variational Inference</a></li>
          <li><a href="#12--parameterizing-conditional-distributions-with-neural-networks" id="markdown-toc-12--parameterizing-conditional-distributions-with-neural-networks">1.2  Parameterizing Conditional Distributions with Neural Networks</a></li>
          <li><a href="#13--directed-graphical-models-and-neural-networks" id="markdown-toc-13--directed-graphical-models-and-neural-networks">1.3  Directed Graphical Models and Neural Networks</a></li>
          <li><a href="#14--learning-in-fully-observed-models-with-neural-nets" id="markdown-toc-14--learning-in-fully-observed-models-with-neural-nets">1.4  Learning in Fully Observed Models with Neural Nets</a>            <ul>
              <li><a href="#141-dataset" id="markdown-toc-141-dataset">1.4.1 Dataset</a></li>
              <li><a href="#142-maximum-likelihood-and-minibatch-sgd" id="markdown-toc-142-maximum-likelihood-and-minibatch-sgd">1.4.2 Maximum Likelihood and Minibatch SGD</a></li>
            </ul>
          </li>
          <li><a href="#15-intractabilities" id="markdown-toc-15-intractabilities">1.5 Intractabilities</a></li>
        </ul>
      </li>
      <li><a href="#2-variational-autoencoders" id="markdown-toc-2-variational-autoencoders">2. Variational Autoencoders</a>        <ul>
          <li><a href="#21-loss-function-evidence-lower-bound" id="markdown-toc-21-loss-function-evidence-lower-bound">2.1 Loss function (Evidence Lower Bound)</a></li>
          <li><a href="#22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick" id="markdown-toc-22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick">2.2 Stochastic Gradient-Based Optimization of the ELBO (Reparameterization Trick)</a>            <ul>
              <li><a href="#221-reparameterization-trick" id="markdown-toc-221-reparameterization-trick">2.2.1 Reparameterization Trick</a></li>
              <li><a href="#222-computation-of-inference-distribution" id="markdown-toc-222-computation-of-inference-distribution">2.2.2 Computation of Inference Distribution</a></li>
            </ul>
          </li>
          <li><a href="#23-estimation-of-the-marginal-likelihood" id="markdown-toc-23-estimation-of-the-marginal-likelihood">2.3 Estimation of the Marginal Likelihood</a></li>
          <li><a href="#25-summary-of-training" id="markdown-toc-25-summary-of-training">2.5 Summary of Training</a></li>
          <li><a href="#251-data-preposscessing" id="markdown-toc-251-data-preposscessing">2.5.1 Data Preposscessing</a>            <ul>
              <li><a href="#252-network-architecture" id="markdown-toc-252-network-architecture">2.5.2 Network Architecture</a></li>
            </ul>
          </li>
          <li><a href="#24-challenges" id="markdown-toc-24-challenges">2.4 Challenges</a>            <ul>
              <li><a href="#241-optimization-issues" id="markdown-toc-241-optimization-issues">2.4.1 Optimization issues</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
    </ul>
  </li>
</ul>

<h1 id="generative-model-part-1a-survey-on-variational-autoencoders">Generative Model Part 1ï¼šA Survey on Variational Autoencoders</h1>

<p>â€œVAE marrys graphical models and deep learningâ€ â€”Diederik P. Kingma</p>

<p>æ­£å¦‚VAEçš„ä½œè€…åœ¨ã€ŠAn Introduction to Variational Autoencodersã€‹æ‰€è¨€ï¼ŒVAEç»“åˆäº†æ¦‚ç‡æ¨¡å‹ï¼Œå›¾æ¨¡å‹ï¼Œç¥ç»ç½‘ç»œï¼ŒæˆåŠŸçš„å®ç°äº†å¯¹äºå¤æ‚åˆ†å¸ƒçš„æ‹Ÿåˆï¼Œå»ºç«‹åœ¨ç¥ç»ç½‘ç»œè¿™æ ·çš„æ ‡å‡†çš„ï¼Œé«˜æ•ˆçš„æ‹Ÿåˆå…¶ä¸Šï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œè®­ç»ƒ</p>

<h2 id="1--introduction">1.  Introduction</h2>

<p>è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œä¸»è¦å™è¿°å…³äºæ¦‚ç‡æ¨¡å‹ï¼Œç¥ç»ç½‘ç»œï¼Œå›¾æ¨¡å‹çš„ä¸€äº›åŸºç¡€</p>

<h3 id="11--probabilistic-models-and-variational-inference">1.1  Probabilistic Models and Variational Inference</h3>

<p>æ¦‚ç‡æ¨¡å‹ä¸­ï¼Œ\(\mathbf{X}\) ä»£è¡¨æ‰€æœ‰è§‚å¯Ÿæ•°æ®çš„é›†åˆï¼Œä¹Ÿæ˜¯éœ€è¦è”åˆåˆ†å¸ƒå»ºæ¨¡çš„å¯¹è±¡ï¼Œéœ€è¦å»é€¼è¿‘çœŸå®çš„åˆ†å¸ƒ \(p^{*}(\mathbf{x})\) ï¼Œåˆ©ç”¨ä¸€ä¸ªç”± \(\boldsymbol{\theta}\) æ§åˆ¶çš„æ¨¡å‹ï¼š</p>

\[\mathbf{x} \sim p_{\boldsymbol{\theta}}(\mathbf{x})\]

<p>è€Œæ¦‚ç‡æ¨¡å‹ä¸­ï¼Œå­¦ä¹ çš„æœ¬è´¨æ˜¯ï¼Œå¯»æ‰¾æœ€é€‚åˆçš„å‚æ•° \(\boldsymbol{\theta}\) æ¥é€¼è¿‘çœŸå®åˆ†å¸ƒ  \(p^{*}(\mathbf{x})\)</p>

\[p_{\boldsymbol{\theta}}(\mathbf{x}) \approx p^{*}(\mathbf{x})\]

<p>å› æ­¤ï¼Œå¸Œæœ› \(p^{*}(\mathbf{x})\) è¶³å¤Ÿçµæ´»ä»¥é€¼è¿‘ä¸€ä¸ªè¶³å¤Ÿç²¾ç¡®çš„æ¨¡å‹</p>

<p>ä¸Šè¿°æ¨¡å‹è¢«ç§°ä¸ºéæ¡ä»¶æ¦‚ç‡æ¨¡å‹ï¼Œä¸æ­¤ç›¸å¯¹åº”çš„ï¼Œè¿˜æœ‰ä¸€ç§æ¨¡å‹è¢«ç§°ä¸ºæ¡ä»¶æ¦‚ç‡æ¨¡å‹ï¼š</p>

\[p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x}) \approx p^{*}(\mathbf{y} \mid \mathbf{x})\]

<p>å¸¸è¢«ç”¨äºå›å½’ï¼Œåˆ†ç±»é—®é¢˜ï¼Œå°½ç®¡æ¡ä»¶æ¦‚ç‡æ¨¡å‹ä¸éæ¡ä»¶æ¦‚ç‡æ¨¡å‹ä¼šåœ¨ç†è®ºä¸Šï¼Œæ˜¯ä¸¤ç§æ¦‚ç‡æ¨¡å‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œéæ¡ä»¶æ¦‚ç‡æ¨¡å‹åœ¨å®è·µä¸Šï¼Œæ˜¯ç”±è§‚å¯Ÿæ•°æ®é›† \(\mathbf{X}\) å»å»ºç«‹ \(p_{\boldsymbol{\theta}}(\mathbf{x})\) ï¼Œå› æ­¤ä¸æ¡ä»¶æ¦‚ç‡æ¨¡å‹æ˜¯ç­‰ä»·çš„ï¼Œåœ¨åé¢ä¼šè¯¦ç»†å™è¿°ã€‚</p>

<h3 id="12--parameterizing-conditional-distributions-with-neural-networks">1.2  Parameterizing Conditional Distributions with Neural Networks</h3>

<p>å°½ç®¡æå‡ºäº†ç†è®ºä¸Šçš„æ¨¡å‹ï¼Œä½†æ˜¯éœ€è¦å…·ä½“çš„æ¨¡å‹å»è¿›è¡Œè®¡ç®—ï¼Œæ¦‚ç‡æ¨¡å‹çš„å»ºæ¨¡æ‰ç®—å®Œæˆ</p>

<p>å¯å¾®ç¥ç»ç½‘ç»œï¼Œæˆ–è€…è¯´ç¥ç»ç½‘ç»œï¼Œæ˜¯å¸¸ç”¨çš„è®¡ç®—å¯è¡Œçš„ï¼Œæ³›åŒ–æ€§å¥½çš„æ–¹æ¡ˆï¼Œä½œä¸ºå‡½æ•°æ‹Ÿåˆå™¨ï¼ŒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œä¸€æ ·èƒ½åšåˆ°å¯ä»¥å­¦ä¹ æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚å› æ­¤åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¦‚ç‡æ¨¡å‹ï¼Œç”±äºå…¶è®¡ç®—å¯è¡Œæ€§ï¼Œä»¥åŠå¯¹äºç¥ç»ç½‘ç»œæœ‰ç›¸å½“å¥½çš„ä¼˜åŒ–æ–¹æ¡ˆå¦‚éšæœºæ¢¯åº¦ä¸‹é™ï¼Œå› æ­¤ä½œä¸ºæ–‡ä¸­çš„å…·ä½“æ¨¡å‹ï¼Œå†™ä½œ \(NeuralNet(.)\)</p>

<p>æ¯”å¦‚åˆ©ç”¨ç¥ç»ç½‘ç»œå»å»ºæ¨¡ä¸€ä¸ªå›¾ç‰‡åˆ†ç±»æ¨¡å‹ \(p_{\boldsymbol{\theta}}(y \mid \mathbf{x})\) ï¼Œ\(y\) ä»£è¡¨ç±»ï¼Œ \(\mathbf{x}\) ä»£è¡¨å›¾ç‰‡ï¼š</p>

\[\begin{aligned}
\mathbf{p} &amp;=\text { NeuralNet }(\mathbf{x}) \\
p_{\boldsymbol{\theta}}(y \mid \mathbf{x}) &amp;=\text { Categorical }(y ; \mathbf{p})
\end{aligned}\]

<p>ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†ä½¿å¾—è¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œä¼šåˆ©ç”¨å¦‚ softmax å±‚ç­‰ä½œä¸º \(NeuralNet(.)\) çš„æœ€åä¸€å±‚æ¥è§„èŒƒè¾“å‡ºä½¿ \(\sum_{i} p_{i}=1\)</p>

<h3 id="13--directed-graphical-models-and-neural-networks">1.3  Directed Graphical Models and Neural Networks</h3>

<p>åˆ°å…·ä½“çš„æ¦‚ç‡æ¨¡å‹å»ºæ¨¡æ—¶ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾å»ºç«‹æ¦‚ç‡å˜é‡ä¹‹é—´çš„è”ç³»ï¼Œç§°ä¸ºæ¦‚ç‡å›¾æ¨¡å‹ï¼Œæ¯”å¦‚ä¸‹å›¾è¿™æ ·çš„ï¼š</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/440px-Graph_model.svg.png" alt="440px-Graph_model.svg" style="zoom:50%;" /></p>

<p>äºæ˜¯åœ¨æœ‰å‘æ— ç¯å›¾å»ºæ¨¡ä¸‹çš„æ¦‚ç‡æ¨¡å‹ï¼Œå…¶è”åˆæ¦‚ç‡å¯ä»¥å†™æˆï¼š</p>

\[p_{\boldsymbol{\theta}}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{M}\right)=\prod_{j=1}^{M} p_{\boldsymbol{\theta}}\left(\mathbf{x}_{j} \mid P a\left(\mathbf{x}_{j}\right)\right)\]

<p>\(P a\left(\mathbf{x}_{j}\right)\) æŒ‡ä»£çš„æ˜¯èŠ‚ç‚¹ \(j\) çš„æ‰€æœ‰çˆ¶èŠ‚ç‚¹ï¼Œè€Œå¯¹äºæ ¹èŠ‚ç‚¹ï¼Œå®šä¹‰å…¶ \(P a\left(\mathbf{x}_{j}\right)\) ä¸ºç©ºé›†</p>

<p>ä¸ºäº†å…·ä½“çš„å‚æ•°åŒ–æœ‰å‘æ— ç¯å›¾æ¦‚ç‡æ¨¡å‹ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡ï¼Œç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯ä¸€ä¸ªéšæœºå˜é‡ \(\mathbf{x}\) çš„çˆ¶èŠ‚ç‚¹  \(P a(\mathbf{x})\)ï¼Œè¾“å‡ºçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒ \(\eta\) ï¼š</p>

\[\begin{aligned}
\boldsymbol{\eta} &amp;=\operatorname{Neural} \operatorname{Net}(P a(\mathbf{x})) \\
p_{\boldsymbol{\theta}}(\mathbf{x} \mid P a(\mathbf{x})) &amp;=p_{\boldsymbol{\theta}}(\mathbf{x} \mid \boldsymbol{\eta})
\end{aligned}\]

<p>ä¸‹é¢å™è¿°å¦‚ä½•å­¦ä¹ è¿™æ ·çš„æ¨¡å‹çš„å‚æ•°</p>

<h3 id="14--learning-in-fully-observed-models-with-neural-nets">1.4  Learning in Fully Observed Models with Neural Nets</h3>

<p>åœ¨æœ‰å‘æ— ç¯å›¾æ¦‚ç‡æ¨¡å‹ä¸­ï¼Œå¦‚æœåœ¨å›¾ä¸­æ‰€æœ‰çš„éšæœºå˜é‡éƒ½åœ¨æ•°æ®ä¸­è¢«è§‚å¯Ÿåˆ°äº†ï¼Œé‚£ä¹ˆéœ€è¦åšåˆ°å°±æ˜¯å¸¸è§„çš„ MLE ä¼˜åŒ–ï¼Œè®¡ç®—ï¼Œå¾®åˆ†ï¼ˆStraightforwardï¼ï¼‰</p>

<h4 id="141-dataset">1.4.1 Dataset</h4>

<p>å¯¹é‡‡æ ·è¿‡ç¨‹æœ‰ \(i.i.d\) å‡è®¾</p>

\[\mathcal{D}=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)}\right\} \equiv\left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N} \equiv \mathbf{x}^{(1: N)}\]

<p>å› æ­¤ï¼Œåˆ©ç”¨å– \(\log\) åˆ†è§£è¿ä¹˜æœ‰ï¼š</p>

\[\log p_{\boldsymbol{\theta}}(\mathcal{D})=\sum_{\mathbf{x} \in \mathcal{D}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\]

<h4 id="142-maximum-likelihood-and-minibatch-sgd">1.4.2 Maximum Likelihood and Minibatch SGD</h4>

<p>åœ¨ ML æ ‡å‡†ä¸­ï¼Œä¼˜åŒ–åœ¨ç»™å‡ºæ ‡å‡†åï¼Œå–å¯»æ‰¾æœ€ä¼˜å‚æ•° \(\theta^*\) ä½¿å¾—æ ‡å‡†æœ€ä¼˜ï¼Œæ¯”å¦‚</p>

\[\theta^* = \arg\min_{\theta} \sum_{\mathbf{x} \in \mathcal{D}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\]

<p>è€Œå¸¸ç”¨çš„æ±‚è§£ç®—æ³•æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆSGDï¼‰ï¼Œå½“åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œä¸€æ¬¡æ¢¯åº¦è®¡ç®—çš„è¯ï¼Œ ç§°ä¸º batch gradient descentï¼Œä½†æ˜¯å½“æ•°æ®é›†å¾ˆå¤§çš„æ—¶å€™ï¼Œæ›´åŠ é€‚åˆç”¨ minibatch SGD æ¥å¤„ç†</p>

<h3 id="15-intractabilities">1.5 Intractabilities</h3>

<p>DLVMï¼Œæ·±åº¦éšå˜é‡æ¨¡å‹ï¼Œå»ºç«‹åœ¨éšç©ºé—´ä¸Šï¼Œéšç©ºé—´ä¸­çš„å˜é‡æ˜¯éš¾ä»¥ç›´æ¥è§‚å¯Ÿå¾—åˆ°çš„ï¼Œå¾€å¾€å¾ˆéš¾åœ¨æ•°æ®ä¸­ç›´æ¥ä½“ç°ï¼Œä½†æ˜¯ç¡®å®å­˜åœ¨ç€éšç©ºé—´ï¼Œä»¥mnistæ‰‹å†™ä½“æ•°æ®åº“æ¥è¯´çš„è¯ï¼Œå°½ç®¡æ‰‹å†™ä½“çš„ç»´åº¦æ—¶ \(28 \times 28\) ç»´ï¼Œä½†æ˜¯å®é™…ä¸Šåœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œåœ¨æ‰‹å†™è¿‡ç¨‹ä¸­ï¼Œå¤§è„‘å»æ„å»ºæ‰‹å†™ä½“æ•°å­—çš„ç»“æ„æ—¶ï¼Œæƒ³çš„æ˜¯ç¬”ç”»å¤šé‡ï¼Œå¼¯æŠ˜æ€ä¹ˆå†™ï¼Œè¿™éƒ¨åˆ†æ€æƒ³è¿‡ç¨‹å¯ä»¥å½’çº³åˆ°éšç©ºé—´å»ºæ¨¡ä¸Š</p>

<p>ç”Ÿæˆæ¨¡å‹ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œç›´æ¥å»ºç«‹ \(p(\mathbf{x})\) å¾ˆéš¾ï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ä¸€ç»„éšå˜é‡æ¥å»ºæ¨¡ï¼š</p>

\[p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z} = \int p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) d \mathbf{z}\]

<p>ä½†æ˜¯åœ¨å®é™…å»ºæ¨¡ä¸­ï¼Œä¸Šè¿°çš„æ¨¡å‹æ—¶å¾ˆéš¾è®¡ç®—çš„ï¼Œå³ DLVM çš„ä¸å¯å¤„ç†æ€§</p>

<p>æœ€å¤§ä¼¼ç„¶ä¼°è®¡åœ¨ DLVM éœ€è¦è§£æçš„ï¼Œæˆ–è€…ä¼°è®¡å‡º \(p_{\boldsymbol{\theta}}(\mathbf{x})\)ï¼Œå†é€šè¿‡ä¼˜åŒ– \(p_{\boldsymbol{\theta}}(\mathbf{x})\) æ‰¾å‡º \(\theta^*\)ï¼Œä½†æ˜¯ç”±äºæ±‚è§£ç§¯åˆ† \(p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z}\) çš„è¿‡ç¨‹ï¼Œè¿™ä¸ªä¸èƒ½ç›´æ¥è§£æçš„åšå‡ºï¼Œæˆ–ä¼°è®¡å‡ºï¼Œé€šè¿‡ MLE ä¼°è®¡æ¥ä¼˜åŒ–æ˜¯ä¸å¯å¤„ç†çš„ï¼ŒåŒæ—¶è¿å¸¦çš„ï¼Œç”±äºåéªŒåˆ†å¸ƒæœ‰è®¡ç®—å…¬å¼ï¼š</p>

\[p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})=\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{x})}\]

<p>å› æ­¤åéªŒåˆ†å¸ƒä¹Ÿæ˜¯ä¸èƒ½è®¡ç®—çš„ï¼Œå¦‚ MAP ç­‰æ–¹æ³•åŒæ ·ä¸èƒ½ä½¿ç”¨ï¼Œä¸ºäº†è§£å†³ä¸å¯å¤„ç†æ€§ï¼Œç”Ÿæˆæ¨¡å‹ä¸­æœ‰å‡ ç±»æ–¹æ¡ˆï¼Œä¸‹é¢ä¸»è¦å…³æ³¨ Variational Autoencoders</p>

<h2 id="2-variational-autoencoders">2. Variational Autoencoders</h2>

<p>è®­ç»ƒå®Œæˆçš„ VAE ï¼Œä»éšç©ºé—´ä¸­é‡‡æ · \(\boldsymbol{z}\)ï¼Œåˆ©ç”¨è§£ç å™¨ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) å»å¤å»ºå‡º \(\boldsymbol{X}\)ï¼Œå…¶æ¨¡å‹å¦‚ä¸‹æ‰€ç¤º</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/latent_encoder_graph.png" alt="latent_encoder_graph" style="zoom:30%;" /></p>

<p>ä½†æ˜¯ç”±äºåéªŒä¸å¯å¤„ç†æ€§ï¼Œç”Ÿæˆæ¨¡å‹å¹¶æ— æ³•ç›´æ¥æ ¹æ®æ•°æ®é›†ç›´æ¥æ±‚è§£ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) ï¼Œæœ‰å‡ ç§è§£å†³åŠæ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚VAE ä»ç„¶ä»æœ€å¤§è¾¹é™…åéªŒ \(p_{\theta}(\mathbf{x})\) çš„æƒ³æ³•å‡ºå‘ï¼Œä½†æ˜¯ä¼˜åŒ–åéªŒçš„ä¸€ä¸ªä¸‹ç•Œæ¥ä½¿åéªŒå˜å¤§ï¼ˆè§åæ–‡ ELBO éƒ¨åˆ†ï¼‰ï¼Œæƒ³æ³•æœ‰å€Ÿé‰´è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAutoencoderï¼‰ï¼Œä½†æ˜¯ç”±äºè¾¹é™…åéªŒ \(p_{\boldsymbol{\theta}}(\mathbf{x})\) ä¸å¯å¤„ç†æ€§ã€‚VAEè¡¥å…¨äº†è‡ªåŠ¨ç¼–ç å™¨çš„å‰åŠéƒ¨åˆ†ï¼Œé€šè¿‡åšä¸€ä¸ªå‚æ•°æ¨æ–­æ¨¡å‹ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) æ¥æ›¿æ¢ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) æ¨¡æ‹Ÿç¼–ç å™¨
\(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \approx p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/VAE-illustration.png" alt="VAE-illustration" style="zoom:30%;" /></p>

<p>æ­£å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¦‚æœå¼ºè°ƒ VAE çš„è‡ªåŠ¨ç¼–ç å™¨çš„å±æ€§ï¼Œåˆ™æ¨æ–­åˆ†å¸ƒ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) æ„æˆäº†ç¼–ç å™¨ï¼Œè§£ç å™¨åˆ™æ˜¯ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)ï¼Œå½“ç„¶å¦‚æœèƒ½å¤Ÿç›´æ¥æ±‚è§£ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) æ˜¯æœ€å¥½çš„ï¼Œä½†æ˜¯ä¸Šè¿°è§£ç å™¨æ˜¯æ— æ³•ç›´æ¥ä¼˜åŒ–çš„ï¼Œå› æ­¤æ‰éœ€è¦å†åšä¸€ä¸ªç¼–ç å™¨çš„æ¨æ–­åˆ†å¸ƒ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ï¼Œæœ‰äº†æ¨æ–­åˆ†å¸ƒåï¼Œå°±å¯ä»¥ä»å…¶ä¸­é‡‡æ · \(z\)ï¼Œå†é•œåƒçš„åˆ©ç”¨è§£ç å™¨ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) æ˜ å°„å›æ•°æ®ç©ºé—´åï¼Œåˆ©ç”¨äº¤å‰ç†µå»ç®—å‡º lossï¼Œåˆ©ç”¨ SGD å»ä¼˜åŒ–ç¼–ç å™¨å’Œè§£ç å™¨çš„å‚æ•° \(\phi,\theta\)</p>

<p>åœ¨å…·ä½“é—®é¢˜ä¸­ï¼Œé€šè¿‡ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ç»“æ„æ¥å»ºæ¨¡æ¨æ–­åˆ†å¸ƒï¼š</p>

\[q_{\phi}(\mathbf{z} \mid \mathbf{x})=q_{\phi}\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{M} \mid \mathbf{x}\right)=\prod_{j=1}^{M} q_{\phi}\left(\mathbf{z}_{j} \mid P a\left(\mathbf{z}_{j}\right), \mathbf{x}\right)\]

<p>ä¸ºäº†å…·ä½“çš„å»ºæ¨¡ä¸Šè¿°é€¼è¿‘ï¼ŒVAE é‡‡ç”¨äº†ä¸¤æ­¥ï¼Œå³ \(Evidence \; Lower \; Bound\) æ¥ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼ŒåŒæ—¶åœ¨å…·ä½“ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡‡ç”¨äº† \(Reparameterization\; Trick\) æ¥è§£å†³æ¢¯åº¦çš„æ±‚è§£é—®é¢˜</p>

<h3 id="21-loss-function-evidence-lower-bound">2.1 Loss function (Evidence Lower Bound)</h3>

<p>è¾¹é™…åéªŒ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) ä¸ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ç‹¬ç«‹ï¼Œå› æ­¤é€šè¿‡é‡å†™æˆæœŸæœ›ï¼Œå¯ä»¥æ¨å¯¼å‡º \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) ç­‰ä»·äº ELBO åŠ ä¸€ä¸ªKLæ•£åº¦</p>

\[\begin{aligned}
\log p_{\boldsymbol{\theta}}(\mathbf{x}) 
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x})\right] 
\\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;= \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;= \underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\\(\mathrm{ELBO})}
+
\underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=D_{K L}\left(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)}

\end{aligned}\]

<p>é‚£ä¹ˆæœ€å¤§åéªŒè¿‘ä¼¼äºä¼˜åŒ– \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) (ELBO)</p>

\[\begin{aligned}
\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x}) 
&amp;= \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]
\\
 &amp;=\log p_{\boldsymbol{\theta}}(\mathbf{x})-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right) \\
&amp; \leq \log p_{\boldsymbol{\theta}}(\mathbf{x})
\end{aligned}\]

<p>è€Œç¬¬äºŒé¡¹KLæ•£åº¦ï¼Œå¾ˆå·§å¦™çš„åŒæ—¶è¡¨è¾¾äº†ä¸¤ä¸ªè·ç¦»ï¼š</p>

<ol>
  <li>ELBOå’Œè¾¹é™…åéªŒä¸­é—´çš„è·ç¦»</li>
  <li>æ¨æ–­åˆ†å¸ƒ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ä¸åéªŒ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) çš„è·ç¦»</li>
</ol>

<p>ä¼˜åŒ–ç›®æ ‡ä»è¾¹é™…åéªŒåˆ°  \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) ï¼Œæ¯”èµ·ä¸å¯å¤„ç†çš„è¾¹é™…åéªŒï¼Œ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) èƒ½å¤Ÿé€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ï¼Œç†è®ºä¸Šæ¥è¯´ä¼˜åŒ– \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) å¯ä»¥åŒæ—¶è¾¾æˆä¸¤ä¸ªç›®æ ‡ï¼š</p>

<ol>
  <li>ä¼˜åŒ–è¾¹é™…åˆ†å¸ƒ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) çš„å˜åˆ†ä¸‹ç•Œï¼Œä½¿å¾—è¾¹é™…åˆ†å¸ƒä¸å°ï¼Œè¿‘ä¼¼çš„è¾¾æˆæœ€å¤§ä¼¼ç„¶çš„æ•ˆæœ</li>
  <li>ä½¿ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) ç¬¬äºŒé¡¹ \(-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)\) å˜å¤§ï¼Œå³ä½¿æ¨æ–­åˆ†å¸ƒå’ŒåéªŒåˆ†å¸ƒçš„è·ç¦»å˜å°ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´å‡†ç¡®çš„ç¼–ç å™¨åˆ†å¸ƒï¼Œä¸€æ—¦èƒ½å¤Ÿå‡†ç¡®å»ºæ¨¡ç¼–ç å™¨åˆ†å¸ƒï¼Œæœ‰åŠ©äºä¼˜åŒ–è§£ç å™¨åˆ†å¸ƒ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</li>
</ol>

<h3 id="22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick">2.2 Stochastic Gradient-Based Optimization of the ELBO (Reparameterization Trick)</h3>

<h4 id="221-reparameterization-trick">2.2.1 Reparameterization Trick</h4>

<p>ç»™ä¸€ä¸ª \(i.i.d\) æ•°æ®é›† \(\mathcal{D}\)ï¼Œæ•°æ®é›†äº§ç”Ÿ \(\mathcal{L}_{\theta, \phi}(\mathcal{D})\) ç­‰ä»·äº \(\sum_{\mathbf{x} \in \mathcal{D}} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)ï¼Œé‚£ä¹ˆä¸‹é¢çš„è®¨è®ºä¸­ï¼Œå°±å˜æˆè®¡ç®— \(\mathcal{L}_{\theta, \phi}(\mathbf{x})\)ï¼Œè¿™æ ·æ˜¯ç­‰ä»·çš„ï¼Œé‡‡ç”¨æ¢¯åº¦æ³•å»ä¼˜åŒ– \(\sum_{\mathbf{x} \in \mathcal{D}} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)</p>

<p>æ¢¯åº¦ \(\nabla_{\boldsymbol{\theta}, \phi} \mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) ï¼Œå¯¹äº \(\theta\) æœ‰ï¼š</p>

\[\begin{aligned}
\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) &amp;=\nabla_{\boldsymbol{\theta}} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right] \\

&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right)\right] \\

&amp; = \nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right) \\

&amp;=\nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})\right)
\end{aligned}\]

<p>ä½†æ˜¯é—®é¢˜åœ¨äºï¼Œç”±äºä¹‹å‰å¯¹äº \(\phi\)ï¼Œç”±äº \(z\) æ˜¯ \(\phi\) çš„å‡½æ•°ï¼Œè¿™æ ·æ¢¯åº¦éš¾ä»¥è§£æçš„ç›´æ¥è®¡ç®—</p>

\[\begin{aligned}
\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x}) &amp;=\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right] \\
&amp; \neq \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\nabla_{\phi}\left(\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right)\right]
\end{aligned}\]

<p>VAE æä¾›çš„è§£å†³åŠæ³•æ˜¯å»æ„é€ ä¸€ä¸ª \(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\) æ— åä¼°è®¡ \(\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\) ï¼Œå³å¦ä¸€ä¸ªæŠ€å·§ \(Reparameterization \; Trick\)</p>

<p>\(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\) æ˜¯ç”±äºè®¡ç®— \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) çš„æœŸæœ›ï¼Œå†æ­¤åŸºç¡€ä¸Šç®— \(\phi\) çš„åå¯¼æ•°å°±å¯¼è‡´éš¾ä»¥è§£æçš„è®¡ç®—ï¼Œè¿™æ ·å°±æ— æ³•æ‰§è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œäºæ˜¯ä½œè€…åœ¨ \(Reparameterization \; Trick\) ä¸­è®¾è®¡äº†ä¸€ä¸ªæ— åç»Ÿè®¡é‡æ¥ä¼°è®¡ \(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)ï¼š</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/Reparameterization-Trick.png" alt="Reparameterization-Trick" style="zoom:40%;" /></p>

<p>å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ„é€ ä¸€ä¸ªå™ªå£°å˜é‡ \(\epsilon\) æœ‰åˆ†å¸ƒ \(p(\boldsymbol{\epsilon})\)ï¼Œå®šä¹‰æ–°çš„æ˜ å°„å…³ç³»ï¼š\(\mathbf{z}=\mathbf{g}(\boldsymbol{\epsilon}, \boldsymbol{\phi}, \mathbf{x})\) æ»¡è¶³</p>

\[\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[f(\mathbf{z})]=\mathbb{E}_{p(\boldsymbol{\epsilon})}[f(\mathbf{z})]\]

<p>è¿™æ ·å°±å¯ä»¥æŠŠ \(z\) çš„éšæœºæ€§è½¬ç§»åˆ° \(\epsilon\) ä¸Šï¼Œå…ˆåšä¸€ç§æŠ½è±¡ä¸€äº›çš„é—®é¢˜è®¾å®šï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¯¹ \(\phi\) æ±‚å¯¼ï¼Œå¯ä»¥æœ‰ä¸€ä¸ª \(\nabla_{\boldsymbol{\phi}} f(\mathbf{z})\) çš„ä¼°è®¡ï¼š
\(\begin{aligned}
\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[f(\mathbf{z})] &amp;=\nabla_{\phi} \mathbb{E}_{p(\boldsymbol{\epsilon})}[f(\mathbf{z})] \\
&amp;=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\phi}} f(\mathbf{z})\right] \\
&amp; \simeq \nabla_{\boldsymbol{\phi}} f(\mathbf{z})
\end{aligned}\)</p>

<p>æ›´è¿›ä¸€æ­¥ï¼Œä»£å…¥å…·ä½“çš„å‡½æ•° \(f(z) = \log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\)ï¼Œæ„é€ ä¸€ä¸ªè’™ç‰¹å¡æ´›ä¼°è®¡
\(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x})\)ï¼š
\(\begin{aligned}
\boldsymbol{\epsilon} &amp; \sim p(\boldsymbol{\epsilon})
\\
\mathbf{z} &amp;=\mathbf{g}(\boldsymbol{\phi}, \mathbf{x}, \boldsymbol{\epsilon}) 
\\
\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) &amp;=\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})
\end{aligned}\)</p>

<p>å¯ä»¥è¯æ˜ \(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x})\) æ˜¯æ— åçš„ï¼š</p>

\[\begin{aligned}
\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\theta}, \phi} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\right] &amp;=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right)\right] \\
&amp;=\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]\right) \\
&amp;=\nabla_{\boldsymbol{\theta}, \phi} \mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})
\end{aligned}\]

<p>åœ¨å®é™…è®¡ç®—ä¸­ï¼Œç±»ä¼¼SGDçš„æ€æƒ³ä¸€æ ·ï¼Œåˆ©ç”¨å¤šæ¬¡æ¯”è¾ƒâ€œæ¨¡ç³Šâ€çš„è®¡ç®—æ¥æ›¿æ¢å•æ¬¡â€œç²¾å‡†â€çš„è®¡ç®—ï¼Œå³ç›´æ¥è®¡ç®—ï¼š
\(\nabla_{\boldsymbol{\theta, \phi}}\log p(x \mid z)+\log p(z)-\log q(z \mid x)
\\
z \text{ sampled from inference distribution } q(z \mid x) \text{ once }\)
å½“ç„¶è¿™æ ·è¯´å¯èƒ½è¿˜æ˜¯æœ‰ä¸€äº›æŠ½è±¡ï¼Œä¸‹é¢ä¼šå…·ä½“çš„ä¸¾ä¸€ä¸ªå…·ä½“çš„éšç©ºé—´ \(z\) å’Œç›¸åº”çš„ \(\epsilon\) çš„ä¾‹å­</p>

<h4 id="222-computation-of-inference-distribution">2.2.2 Computation of Inference Distribution</h4>

<p>åœ¨è®¡ç®— \(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) =\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\) çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦è®¡ç®—ç¼–ç å™¨/æ¨æ–­åˆ†å¸ƒ \(\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\)</p>

\[\begin{aligned}
\boldsymbol{\epsilon} &amp; \sim p(\boldsymbol{\epsilon})
\\
\mathbf{z} &amp;=\mathbf{g}(\boldsymbol{\phi}, \mathbf{x}, \boldsymbol{\epsilon}) 
\\
p(\boldsymbol{\epsilon}) &amp;= q_{\phi}(\mathbf{z} \mid \mathbf{x})\left|\operatorname{det}\left(\frac{\partial \mathbf{z}}{\partial \boldsymbol{\epsilon}}\right)\right|

\end{aligned}\]

<p>å› æ­¤å¯ä»¥çœ‹å‡ºï¼Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å¯é€†å˜åŒ– \(g(\epsilon)\) ä¼šä½¿å¾—è®¡ç®—ç®€åŒ–</p>

\[\log q_{\phi}(\mathbf{z} \mid \mathbf{x})=\log p(\boldsymbol{\epsilon})-\log \left|\operatorname{det}\left(\frac{\partial \mathbf{z}}{\partial \boldsymbol{\epsilon}}\right)\right|\]

<p>ä¸‹é¢ä¸¾ä¸€ä¸ªå…·ä½“è®¡ç®—ä¸­æ„é€ æ¨æ–­åˆ†å¸ƒ \(\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\) å’Œ \(Reparameterization \; Trick\) çš„ä¾‹å­ï¼š</p>

<p>éšç©ºé—´ \(z\) æœä»é«˜æ–¯åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°é€šè¿‡ç¼–ç å™¨è®¡ç®—å¾—åˆ°ï¼š</p>

\[\begin{aligned}
q_{\phi}(\mathbf{z} \mid \mathbf{x})  &amp;= \mathcal{N}(\mathbf{z} ; \boldsymbol{\mu}, \operatorname{diag}\left(\sigma^{2}\right))
\\
(\boldsymbol{\mu}, \log \boldsymbol{\sigma}) &amp;=\text { EncoderNeuralNet}_{\boldsymbol{\phi}}(\mathbf{x}) 
\\
q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) &amp;=\prod_{i} q_{\boldsymbol{\phi}}\left(z_{i} \mid \mathbf{x}\right)=\prod_{i} \mathcal{N}\left(z_{i} ; \mu_{i}, \sigma_{i}^{2}\right)
\end{aligned}\]

<p>é‡‡ç”¨ \(Reparameterization \; Trick\)ï¼š</p>

\[\begin{array}{l}
\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}) \\
\mathbf{z}=\mu+\sigma \odot \epsilon
\end{array}\]

<p>å› æ­¤å°±ä½¿å¾— \(z\) å˜æˆäº†ä¸€ä¸ªéšåˆ†å¸ƒï¼Œå°±å¯ä»¥è®¡ç®—åéªŒåˆ†å¸ƒ \(\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ä¸ºï¼š
\(\begin{aligned}
\log q_{\phi}(\mathbf{z} \mid \mathbf{x}) &amp;=\log p(\boldsymbol{\epsilon})-\log d_{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon}) \\
&amp;=\sum_{i} \log \mathcal{N}\left(\epsilon_{i} ; 0,1\right)-\log \sigma_{i}
\end{aligned}\)</p>

<h3 id="23-estimation-of-the-marginal-likelihood">2.3 Estimation of the Marginal Likelihood</h3>

<p>è¾¹é™…ä¼¼ç„¶å¯ä»¥åˆ©ç”¨æ¨æ–­åˆ†å¸ƒå»æ„é€ ï¼š</p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x})=\log \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) / q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]\]

<p>è€Œåœ¨æ•°å€¼ä¸Šï¼Œä¸Šå¼å¯ä»¥åˆ©ç”¨è’™ç‰¹å¡æ´›æ³•è¿›è¡Œä¼°è®¡ï¼š</p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x}) \approx \log \frac{1}{L} \sum_{l=1}^{L} p_{\boldsymbol{\theta}}\left(\mathbf{x}, \mathbf{z}^{(l)}\right) / q_{\boldsymbol{\phi}}\left(\mathbf{z}^{(l)} \mid \mathbf{x}\right)\]

<p>å½“ \(L=1\) æ—¶ï¼Œè¿™ä¸ªå¼å­å°±æ˜¯ \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\)</p>

<h3 id="25-summary-of-training">2.5 Summary of Training</h3>

<p>VAEï¼Œå’ŒGANä¸€æ ·ï¼Œæœ€åé€šè¿‡ä»éšç©ºé—´é‡‡æ ·ï¼Œç„¶åç”¨è§£ç å™¨æ˜ å°„åˆ°æ•°æ®ç©ºé—´ï¼Œç”Ÿæˆæ•°æ®ï¼Œä½†æ˜¯æ¯”èµ·GANé€šè¿‡å»è¿­ä»£è®­ç»ƒç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„ç»“æ„ï¼Œæœ€åæŠŠç”Ÿæˆå™¨å½“ä½œæ˜ å°„ï¼ŒVAEæ›´åŠ å¼ºè°ƒå¯¹äºæ•°æ®åˆ†å¸ƒçš„ç†è§£</p>

<ul>
  <li>
    <p>VAE å‡è®¾éšç©ºé—´ \(z\) æœä»ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ˆå¼ºè°ƒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›é€‰æ‹©åŠä¸‰è§’ä½œä¸ºé«˜æ–¯åˆ†å¸ƒæ–¹å·®ï¼Œåä¹‹é€‰æ‹©å¯¹è§’çŸ©é˜µä½œä¸ºæ–¹å·®ï¼‰ï¼Œåˆ©ç”¨ç¼–ç å™¨æŠŠ \(x\) ä»æ•°æ®ç©ºé—´æ˜ å°„åˆ°éšç©ºé—´ï¼Œè¾“å‡ºéšåˆ†å¸ƒçš„ \(\mu,\sigma\)</p>
  </li>
  <li>
    <p>é€šè¿‡ \(Reparameterization \;trick\) æŠŠ \(z\) çš„éšæœºæ€§è½¬åˆ° \(\epsilon\) ä¸Šï¼Œé‡‡æ · \(z\)</p>
  </li>
  <li>
    <p>\(z\) ç»è¿‡è§£ç å™¨ï¼Œåˆ©ç”¨äº¤å‰ç†µè®¡ç®—å‡ºåéªŒåˆ†å¸ƒ \(p(x \mid z)\)ï¼Œæœ€åè®¡ç®—äº§ç”ŸæŸå¤±è¿›è¡Œåå‘ä¼ æ’­
\(\mathcal{L}_{\theta, \phi}(\mathcal{x}) = \log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) +\log p(\mathbf{z}) -\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \quad (L=1 )\)</p>
  </li>
</ul>

<p>ä¸‹é¢å€Ÿç”¨ Tensorflow.org å¼€æºçš„ <a href="https://www.tensorflow.org/tutorials/generative/cvae">Convolutional Variational Autoencoder</a> çš„å®ç°ï¼ˆä¼˜ç¾ï¼ï¼‰</p>

<h3 id="251-data-preposscessing">2.5.1 Data Preposscessing</h3>

<p>æ•°æ®é¢„å¤„ç†ä¸­ï¼Œå¯¹å›¾åƒè¿›è¡Œç°åº¦-&gt;é»‘ç™½å¤„ç†ï¼Œè¿™æ ·æœ€åè¾“å‡ºåˆ©ç”¨ä¼¯åŠªåˆ©åˆ†å¸ƒè¿›è¡Œè¾“å‡ºå³å¯ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_images</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
  <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mf">255.</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">images</span> <span class="o">&gt;</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>

<span class="n">train_images</span> <span class="o">=</span> <span class="n">preprocess_images</span><span class="p">(</span><span class="n">train_images</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">preprocess_images</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>

</code></pre></div></div>

<h4 id="252-network-architecture">2.5.2 Network Architecture</h4>

<p>CVAEç½‘ç»œç»“æ„æ˜¯ä¸€ä¸ªä¸€å¯¹è€¦åˆçš„è§£ç å™¨å’Œç¼–ç å™¨ï¼Œè§£ç å™¨ç”±ä¸¤å±‚å·æœºå±‚å’Œä¸€å±‚å…¨è”æ¥å±‚æ„æˆï¼Œè§£ç å™¨æŠŠæ•°æ®æ˜ å°„åˆ°éšç©ºé—´ä¸Šï¼ˆè¾“å‡ºéšç©ºé—´çš„å‚æ•°ï¼‰ï¼Œè¿™é‡Œå‡è®¾ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒä½œä¸ºéšç©ºé—´çš„åˆ†å¸ƒï¼Œä¸ºäº†å®ç° \(Reparameterization \; Trick\) åˆ™ç»™å‡ºäº†å¦ä¸€ä¸ªæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„å‚æ•° \(\epsilon\)ï¼š
\(z=\mu+\sigma \odot \epsilon\)
ç„¶åè§£ç å™¨æ˜¯ç¼–ç å™¨çš„é•œåƒï¼Œé•œåƒçš„å…¨è”æ¥å±‚åæ¥äº†ä¸€å±‚åå·ç§¯å±‚</p>

<p>åˆ©ç”¨è§£ç å™¨å’Œç¼–ç å™¨ï¼Œå¯ä»¥è¿›ä¸€æ­¥å†™å‡º encode(.), decode(.), sample(.),, reparameterize(.) å¹¶å°è£…åˆ° CVAE ç±»ä¸­</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CVAE</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="s">"""Convolutional variational autoencoder."""</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CVAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="c1"># No activation
</span>            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">+</span> <span class="n">latent_dim</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="c1"># No activation
</span>            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>

  <span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">apply_sigmoid</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span>

  <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">mean</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logvar</span> <span class="o">*</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span>

  <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">apply_sigmoid</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_sigmoid</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">probs</span>
    <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></div>

<p>ç„¶åè®¡ç®—æŸå¤±å‡½æ•°çš„è¿‡ç¨‹ä¸­ï¼Œæ–¹å·®å–çš„æ˜¯ \(log var\) é˜²æ­¢æ•°å€¼ä¸Šçš„é—®é¢˜ï¼ŒåéªŒåˆ†å¸ƒåˆ©ç”¨äº¤å‰ç†µè®¡ç®—å³å¯ï¼Œä¸ºäº†åŠ é€Ÿè®¡ç®—ï¼Œåªä¸€æ¬¡é‡‡æ · \(z\) ï¼š
\(\log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) = - H(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}), p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}))\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
  <span class="n">x_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="n">cross_ent</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">x_logit</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
  <span class="n">logpx_z</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">cross_ent</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
  <span class="n">logpz</span> <span class="o">=</span> <span class="n">log_normal_pdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
  <span class="n">logqz_x</span> <span class="o">=</span> <span class="n">log_normal_pdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">logpx_z</span> <span class="o">+</span> <span class="n">logpz</span> <span class="o">-</span> <span class="n">logqz_x</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="24-challenges">2.4 Challenges</h3>

<p>VAEï¼Œå’ŒGANä¸€æ ·ï¼Œæœ€åé€šè¿‡ä»éšç©ºé—´é‡‡æ ·ï¼Œç„¶åç”¨è§£ç å™¨æ˜ å°„åˆ°æ•°æ®ç©ºé—´ï¼Œç”Ÿæˆæ•°æ®ï¼Œä½†æ˜¯å’ŒGANä¸ä¸€æ ·çš„æ˜¯ï¼ŒGANè®­ç»ƒçš„æ˜¯å¯èƒ½æ€§ï¼ˆprobabilityï¼‰ï¼Œå³åˆ©ç”¨åˆ¤åˆ«å™¨å»åˆ¤æ–­ç”Ÿæˆæ•°æ®æ˜¯çœŸå®æ•°æ®çš„å¯èƒ½æ€§ï¼ŒVAEä»ç„¶ä¼šå»è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼ˆdistributionï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸€ç‚¹ä¹Ÿç»™VAEçš„è®­ç»ƒå¸¦æ¥çš„å›°éš¾</p>

<h4 id="241-optimization-issues">2.4.1 Optimization issues</h4>

<p>æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒVAE éƒ½è¦å»ç®—æ•°æ®çš„åéªŒåˆ†å¸ƒ \(p(x \mid z)\) æ¥è¿›è¡Œä¼˜åŒ–ï¼Œä½†æ˜¯ä¸€å¼€å§‹è®­ç»ƒæ—¶ï¼Œè§£ç å™¨çš„æ•ˆæœä¼šå¾ˆå·®ï¼Œæ— æ³•æŠŠéšç©ºé—´ä¸­çš„é‡‡æ · \(z\) æ˜ å°„åˆ°çœŸå®çš„æ•°æ®æµå½¢ä¸Šï¼Œé‚£ä¹ˆæ­¤æ—¶è®¡ç®—å‡ºçš„ \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\) ä¸­ \(\log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) å¾ˆå°ï¼Œä¸ºäº†ä½¿ \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\) å˜å°ï¼Œé‚£ä¹ˆä¼šåˆ°è¾¾ä¸€ä¸ªæ— æ„ä¹‰çš„å‡è¡¡ç‚¹ \(\log p(\mathbf{z}) \approx \log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\)ï¼Œæˆ–è€…è¯´ \(D_{K L}(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z})) \approx 0\)ï¼Œæ­¤æ—¶ç¼–ç å™¨ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) å’Œéšç©ºé—´ \(p(\mathbf{z})\) åˆ†å¸ƒ</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Diederik P. Kingma and Max Welling (2019), â€œAn Introduction to Variational Autoencodersâ€, Foundations and TrendsR in Machine Learning</li>
  <li></li>
  <li>Towards a Deeper Understanding of Variational Autoencoding Models</li>
  <li>Tutorial on Variational Autoencoders</li>
  <li><a href="https://www.tensorflow.org/tutorials/generative/cvae">Convolutional Variational Autoencoder</a></li>
</ol>

:ET