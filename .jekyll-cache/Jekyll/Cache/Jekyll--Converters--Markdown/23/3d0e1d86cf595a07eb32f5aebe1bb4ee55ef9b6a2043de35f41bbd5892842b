I"же<ul class="table-of-content" id="markdown-toc">
  <li><a href="#generative-model-part-2a-survey-on-variational-autoencoder" id="markdown-toc-generative-model-part-2a-survey-on-variational-autoencoder">Generative Model Part 2О╪ A Survey on Variational Autoencoder</a>    <ul>
      <li><a href="#1--introduction" id="markdown-toc-1--introduction">1.  Introduction</a>        <ul>
          <li><a href="#11--probabilistic-models-and-variational-inference" id="markdown-toc-11--probabilistic-models-and-variational-inference">1.1  Probabilistic Models and Variational Inference</a></li>
          <li><a href="#12--parameterizing-conditional-distributions-with-neural-networks" id="markdown-toc-12--parameterizing-conditional-distributions-with-neural-networks">1.2  Parameterizing Conditional Distributions with Neural Networks</a></li>
          <li><a href="#13--directed-graphical-models-and-neural-networks" id="markdown-toc-13--directed-graphical-models-and-neural-networks">1.3  Directed Graphical Models and Neural Networks</a></li>
          <li><a href="#14--learning-in-fully-observed-models-with-neural-nets" id="markdown-toc-14--learning-in-fully-observed-models-with-neural-nets">1.4  Learning in Fully Observed Models with Neural Nets</a>            <ul>
              <li><a href="#141-dataset" id="markdown-toc-141-dataset">1.4.1 Dataset</a></li>
              <li><a href="#142-maximum-likelihood-and-minibatch-sgd" id="markdown-toc-142-maximum-likelihood-and-minibatch-sgd">1.4.2 Maximum Likelihood and Minibatch SGD</a></li>
            </ul>
          </li>
          <li><a href="#15-intractabilities" id="markdown-toc-15-intractabilities">1.5 Intractabilities</a></li>
        </ul>
      </li>
      <li><a href="#2-variational-autoencoders" id="markdown-toc-2-variational-autoencoders">2. Variational Autoencoders</a>        <ul>
          <li><a href="#21-loss-function-evidence-lower-bound" id="markdown-toc-21-loss-function-evidence-lower-bound">2.1 Loss function (Evidence Lower Bound)</a></li>
          <li><a href="#22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick" id="markdown-toc-22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick">2.2 Stochastic Gradient-Based Optimization of the ELBO (Reparameterization Trick)</a>            <ul>
              <li><a href="#221-reparameterization-trick" id="markdown-toc-221-reparameterization-trick">2.2.1 Reparameterization Trick</a></li>
              <li><a href="#222-computation-of-inference-distribution" id="markdown-toc-222-computation-of-inference-distribution">2.2.2 Computation of Inference Distribution</a></li>
            </ul>
          </li>
          <li><a href="#23-estimation-of-the-marginal-likelihood" id="markdown-toc-23-estimation-of-the-marginal-likelihood">2.3 Estimation of the Marginal Likelihood</a></li>
          <li><a href="#25-summary-of-training" id="markdown-toc-25-summary-of-training">2.5 Summary of Training</a>            <ul>
              <li><a href="#251-data-preposscessing" id="markdown-toc-251-data-preposscessing">2.5.1 Data Preposscessing</a></li>
              <li><a href="#252-network-architecture" id="markdown-toc-252-network-architecture">2.5.2 Network Architecture</a></li>
              <li><a href="#253-loss-function" id="markdown-toc-253-loss-function">2.5.3 Loss Function</a></li>
            </ul>
          </li>
          <li><a href="#24-challenges" id="markdown-toc-24-challenges">2.4 Challenges</a>            <ul>
              <li><a href="#241-optimization-issues" id="markdown-toc-241-optimization-issues">2.4.1 Optimization issues</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
    </ul>
  </li>
</ul>

<h1 id="generative-model-part-2a-survey-on-variational-autoencoder">Generative Model Part 2О╪ A Survey on Variational Autoencoder</h1>

<p>Б─°VAE marrys graphical models and deep learningБ─² Б─■Diederik P. Kingma</p>

<p>Ф╜ёЕ╕┌VAEГ └Д╫°Х─┘Е°╗Ц─┼An Introduction to Variational AutoencodersЦ─▀Ф┴─Х╗─О╪▄VAEГ╩⌠Е░┬Д╨├Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Е⌡╬Ф╗║Е·▀О╪▄Г╔·Г╩▐Г╫▒Г╩°О╪▄Ф┬░Е┼÷Г └Е╝·Г▌╟Д╨├Е╞╧Д╨▌Е╓█Ф²┌Е┬├Е╦┐Г └Ф▀÷Е░┬О╪▄Е╩╨Г╚▀Е°╗Г╔·Г╩▐Г╫▒Г╩°Х©≥Ф═╥Г └Ф═┤Е┤├Г └О╪▄И╚≤Ф∙┬Г └Ф▀÷Е░┬Е≥╗Д╦┼О╪▄Д╧÷Е▐╞Д╩╔Е┬╘Г■╗Ф╒╞Е╨╕Д╦▀И≥█Г╝≈ФЁ∙Х©⌡Х║▄Х╝╜Г╩┐</p>

<h2 id="1--introduction">1.  Introduction</h2>

<p>Х©≥Д╦─И┐╗Е┬├Д╦╜О╪▄Д╦╩Х╕│Е▐≥Х©╟Е┘ЁД╨▌Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Г╔·Г╩▐Г╫▒Г╩°О╪▄Е⌡╬Ф╗║Е·▀Г └Д╦─Д╨⌡Е÷╨Г║─</p>

<h3 id="11--probabilistic-models-and-variational-inference">1.1  Probabilistic Models and Variational Inference</h3>

<p>Ф╕┌Г▌┤Ф╗║Е·▀Д╦╜О╪▄\(\mathbf{X}\) Д╩ёХ║╗Ф┴─Ф°┴Х╖┌Е╞÷Ф∙╟Ф█╝Г └И⌡├Е░┬О╪▄Д╧÷Ф≤╞И°─Х╕│Х│■Е░┬Е┬├Е╦┐Е╩╨Ф╗║Г └Е╞╧Х╠║О╪▄И°─Х╕│Е▌╩И─╪Х©▒Г°÷Е╝·Г └Е┬├Е╦┐ \(p^{*}(\mathbf{x})\) О╪▄Е┬╘Г■╗Д╦─Д╦╙Г■╠ \(\boldsymbol{\theta}\) Ф▌╖Е┬╤Г └Ф╗║Е·▀О╪ </p>

\[\mathbf{x} \sim p_{\boldsymbol{\theta}}(\mathbf{x})\]

<p>Х─▄Ф╕┌Г▌┤Ф╗║Е·▀Д╦╜О╪▄Е╜╕Д╧═Г └Ф°╛Х╢╗Ф≤╞О╪▄Е╞╩Ф┴╬Ф°─И─┌Е░┬Г └Е▐┌Ф∙╟ \(\boldsymbol{\theta}\) Ф²╔И─╪Х©▒Г°÷Е╝·Е┬├Е╦┐  \(p^{*}(\mathbf{x})\)</p>

\[p_{\boldsymbol{\theta}}(\mathbf{x}) \approx p^{*}(\mathbf{x})\]

<p>Е⌡═Ф╜╓О╪▄Е╦▄Ф°⌡ \(p^{*}(\mathbf{x})\) Х╤ЁЕ╓÷Г│╣Ф╢╩Д╩╔И─╪Х©▒Д╦─Д╦╙Х╤ЁЕ╓÷Г╡╬Г║╝Г └Ф╗║Е·▀</p>

<p>Д╦┼Х©╟Ф╗║Е·▀Х╒╚Г╖╟Д╦╨И²·Ф²║Д╩╤Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Д╦▌Ф╜╓Г⌡╦Е╞╧Е╨■Г └О╪▄Х©≤Ф°┴Д╦─Г╖█Ф╗║Е·▀Х╒╚Г╖╟Д╦╨Ф²║Д╩╤Ф╕┌Г▌┤Ф╗║Е·▀О╪ </p>

\[p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x}) \approx p^{*}(\mathbf{y} \mid \mathbf{x})\]

<p>Е╦╦Х╒╚Г■╗Д╨▌Е⌡·Е╫▓О╪▄Е┬├Г╠╩И≈╝И╒≤О╪▄Е╟╫Г╝║Ф²║Д╩╤Ф╕┌Г▌┤Ф╗║Е·▀Д╦▌И²·Ф²║Д╩╤Ф╕┌Г▌┤Ф╗║Е·▀Д╪ Е°╗Г░├Х╝╨Д╦┼О╪▄Ф≤╞Д╦╓Г╖█Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Д╫├Ф≤╞Е╝·И≥┘Д╦┼О╪▄И²·Ф²║Д╩╤Ф╕┌Г▌┤Ф╗║Е·▀Е°╗Е╝·Х╥╣Д╦┼О╪▄Ф≤╞Г■╠Х╖┌Е╞÷Ф∙╟Ф█╝И⌡├ \(\mathbf{X}\) Е▌╩Е╩╨Г╚▀ \(p_{\boldsymbol{\theta}}(\mathbf{x})\) О╪▄Е⌡═Ф╜╓Д╦▌Ф²║Д╩╤Ф╕┌Г▌┤Ф╗║Е·▀Ф≤╞Г╜┴Д╩╥Г └О╪▄Е°╗Е░▌И²╒Д╪ Х╞╕Г╩├Е▐≥Х©╟Ц─┌</p>

<h3 id="12--parameterizing-conditional-distributions-with-neural-networks">1.2  Parameterizing Conditional Distributions with Neural Networks</h3>

<p>Е╟╫Г╝║Ф▐░Е┤╨Д╨├Г░├Х╝╨Д╦┼Г └Ф╗║Е·▀О╪▄Д╫├Ф≤╞И°─Х╕│Е┘╥Д╫⌠Г └Ф╗║Е·▀Е▌╩Х©⌡Х║▄Х╝║Г╝≈О╪▄Ф╕┌Г▌┤Ф╗║Е·▀Г └Е╩╨Ф╗║Ф┴█Г╝≈Е╝▄Ф┬░</p>

<p>Е▐╞Е╬╝Г╔·Г╩▐Г╫▒Г╩°О╪▄Ф┬√Х─┘Х╞╢Г╔·Г╩▐Г╫▒Г╩°О╪▄Ф≤╞Е╦╦Г■╗Г └Х╝║Г╝≈Е▐╞Х║▄Г └О╪▄ФЁ⌡Е▄√Ф─╖Е╔╫Г └Ф√╧Ф║┬О╪▄Д╫°Д╦╨Е┤╫Ф∙╟Ф▀÷Е░┬Е≥╗О╪▄Е÷╨Д╨▌Ф╥╠Е╨╕Г╔·Г╩▐Г╫▒Г╩°Г └Ф╗║Е·▀О╪▄Д╦─Ф═╥Х┐╫Е│ Е┬╟Е▐╞Д╩╔Е╜╕Д╧═Ф╕┌Г▌┤Е╞├Е╨╕Е┤╫Ф∙╟Ц─┌Е⌡═Ф╜╓Е÷╨Д╨▌Ф╥╠Е╨╕Г╔·Г╩▐Г╫▒Г╩°Г └Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Г■╠Д╨▌Е┘╤Х╝║Г╝≈Е▐╞Х║▄Ф─╖О╪▄Д╩╔Е▐┼Е╞╧Д╨▌Г╔·Г╩▐Г╫▒Г╩°Ф°┴Г⌡╦Е╫⌠Е╔╫Г └Д╪≤Е▄√Ф√╧Ф║┬Е╕┌И ▐Ф°╨Ф╒╞Е╨╕Д╦▀И≥█О╪▄Е⌡═Ф╜╓Д╫°Д╦╨Ф√┤Д╦╜Г └Е┘╥Д╫⌠Ф╗║Е·▀О╪▄Е├≥Д╫° \(NeuralNet(.)\)</p>

<p>Ф╞■Е╕┌Е┬╘Г■╗Г╔·Г╩▐Г╫▒Г╩°Е▌╩Е╩╨Ф╗║Д╦─Д╦╙Е⌡╬Г┴┤Е┬├Г╠╩Ф╗║Е·▀ \(p_{\boldsymbol{\theta}}(y \mid \mathbf{x})\) О╪▄\(y\) Д╩ёХ║╗Г╠╩О╪▄ \(\mathbf{x}\) Д╩ёХ║╗Е⌡╬Г┴┤О╪ </p>

\[\begin{aligned}
\mathbf{p} &amp;=\text { NeuralNet }(\mathbf{x}) \\
p_{\boldsymbol{\theta}}(y \mid \mathbf{x}) &amp;=\text { Categorical }(y ; \mathbf{p})
\end{aligned}\]

<p>Г┴╧Е┬╚ФЁ╗Ф└▐Г └Ф≤╞О╪▄Д╦╨Д╨├Д╫©Е╬≈Х╬⌠Е┤╨Ф≤╞Д╦─Д╦╙Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Д╪ Е┬╘Г■╗Е╕┌ softmax Е╠┌Г╜┴Д╫°Д╦╨ \(NeuralNet(.)\) Г └Ф°─Е░▌Д╦─Е╠┌Ф²╔Х╖└Х▄┐Х╬⌠Е┤╨Д╫© \(\sum_{i} p_{i}=1\)</p>

<h3 id="13--directed-graphical-models-and-neural-networks">1.3  Directed Graphical Models and Neural Networks</h3>

<p>Е┬╟Е┘╥Д╫⌠Г └Ф╕┌Г▌┤Ф╗║Е·▀Е╩╨Ф╗║Ф≈╤О╪▄Д╫©Г■╗Ф°┴Е░▒Ф≈═Г▌╞Е⌡╬Е╩╨Г╚▀Ф╕┌Г▌┤Е▐≤И┤▐Д╧▀И≈╢Г └Х│■ГЁ╩О╪▄Г╖╟Д╦╨Ф╕┌Г▌┤Е⌡╬Ф╗║Е·▀О╪▄Ф╞■Е╕┌Д╦▀Е⌡╬Х©≥Ф═╥Г └О╪ </p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2О╪ Generative Model Part 2О╪ A Survey on Variational Autoencoders.md/440px-Graph_model.svg.png" alt="440px-Graph_model.svg" style="zoom:50%;" /></p>

<p>Д╨▌Ф≤╞Е°╗Ф°┴Е░▒Ф≈═Г▌╞Е⌡╬Е╩╨Ф╗║Д╦▀Г └Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Е┘╤Х│■Е░┬Ф╕┌Г▌┤Е▐╞Д╩╔Е├≥Ф┬░О╪ </p>

\[p_{\boldsymbol{\theta}}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{M}\right)=\prod_{j=1}^{M} p_{\boldsymbol{\theta}}\left(\mathbf{x}_{j} \mid P a\left(\mathbf{x}_{j}\right)\right)\]

<p>\(P a\left(\mathbf{x}_{j}\right)\) Ф▄┤Д╩ёГ └Ф≤╞Х┼┌Г┌╧ \(j\) Г └Ф┴─Ф°┴Г┬╤Х┼┌Г┌╧О╪▄Х─▄Е╞╧Д╨▌Ф═╧Х┼┌Г┌╧О╪▄Е╝ Д╧┴Е┘╤ \(P a\left(\mathbf{x}_{j}\right)\) Д╦╨Г╘╨И⌡├</p>

<p>Д╦╨Д╨├Е┘╥Д╫⌠Г └Е▐┌Ф∙╟Е▄√Ф°┴Е░▒Ф≈═Г▌╞Е⌡╬Ф╕┌Г▌┤Ф╗║Е·▀О╪▄Е┬╘Г■╗Г╔·Г╩▐Г╫▒Г╩°Е╩╨Ф╗║О╪▄Г╔·Г╩▐Г╫▒Г╩°Г └Х╬⌠Е┘╔Ф≤╞Д╦─Д╦╙И ▐Ф°╨Е▐≤И┤▐ \(\mathbf{x}\) Г └Г┬╤Х┼┌Г┌╧  \(P a(\mathbf{x})\)О╪▄Х╬⌠Е┤╨Г └Ф≤╞Ф╕┌Г▌┤Е┬├Е╦┐ \(\eta\) О╪ </p>

\[\begin{aligned}
\boldsymbol{\eta} &amp;=\operatorname{Neural} \operatorname{Net}(P a(\mathbf{x})) \\
p_{\boldsymbol{\theta}}(\mathbf{x} \mid P a(\mathbf{x})) &amp;=p_{\boldsymbol{\theta}}(\mathbf{x} \mid \boldsymbol{\eta})
\end{aligned}\]

<p>Д╦▀И²╒Е▐≥Х©╟Е╕┌Д╫∙Е╜╕Д╧═Х©≥Ф═╥Г └Ф╗║Е·▀Г └Е▐┌Ф∙╟</p>

<h3 id="14--learning-in-fully-observed-models-with-neural-nets">1.4  Learning in Fully Observed Models with Neural Nets</h3>

<p>Е°╗Ф°┴Е░▒Ф≈═Г▌╞Е⌡╬Ф╕┌Г▌┤Ф╗║Е·▀Д╦╜О╪▄Е╕┌Ф·°Е°╗Е⌡╬Д╦╜Ф┴─Ф°┴Г └И ▐Ф°╨Е▐≤И┤▐И┐╫Е°╗Ф∙╟Ф█╝Д╦╜Х╒╚Х╖┌Е╞÷Е┬╟Д╨├О╪▄И┌ёД╧┬И°─Х╕│Е│ Е┬╟Е╟╠Ф≤╞Е╦╦Х╖└Г └ MLE Д╪≤Е▄√О╪▄Х╝║Г╝≈О╪▄Е╬╝Е┬├О╪┬StraightforwardО╪│О╪┴</p>

<h4 id="141-dataset">1.4.1 Dataset</h4>

<p>Е╞╧И┤┤Ф═╥Х©┤Г╗▀Ф°┴ \(i.i.d\) Е│┤Х╝╬</p>

\[\mathcal{D}=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)}\right\} \equiv\left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N} \equiv \mathbf{x}^{(1: N)}\]

<p>Е⌡═Ф╜╓О╪▄Е┬╘Г■╗Е▐√ \(\log\) Е┬├Х╖ёХ©·Д╧≤Ф°┴О╪ </p>

\[\log p_{\boldsymbol{\theta}}(\mathcal{D})=\sum_{\mathbf{x} \in \mathcal{D}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\]

<h4 id="142-maximum-likelihood-and-minibatch-sgd">1.4.2 Maximum Likelihood and Minibatch SGD</h4>

<p>Е°╗ ML Ф═┤Е┤├Д╦╜О╪▄Д╪≤Е▄√Е°╗Г╩≥Е┤╨Ф═┤Е┤├Е░▌О╪▄Е▐√Е╞╩Ф┴╬Ф°─Д╪≤Е▐┌Ф∙╟ \(\theta^*\) Д╫©Е╬≈Ф═┤Е┤├Ф°─Д╪≤О╪▄Ф╞■Е╕┌</p>

\[\theta^* = \arg\min_{\theta} \sum_{\mathbf{x} \in \mathcal{D}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\]

<p>Х─▄Е╦╦Г■╗Г └Ф╠┌Х╖ёГ╝≈ФЁ∙Ф≤╞И ▐Ф°╨Ф╒╞Е╨╕Д╦▀И≥█Г╝≈ФЁ∙О╪┬SGDО╪┴О╪▄Е╫⌠Е°╗Ф∙╢Д╦╙Ф∙╟Ф█╝И⌡├Д╦┼Х©⌡Х║▄Д╦─Ф╛║Ф╒╞Е╨╕Х╝║Г╝≈Г └Х╞²О╪▄ Г╖╟Д╦╨ batch gradient descentО╪▄Д╫├Ф≤╞Е╫⌠Ф∙╟Ф█╝И⌡├Е╬┬Е╓╖Г └Ф≈╤Е─≥О╪▄Ф⌡╢Е┼═И─┌Е░┬Г■╗ minibatch SGD Ф²╔Е╓└Г░├</p>

<h3 id="15-intractabilities">1.5 Intractabilities</h3>

<p>DLVMО╪▄Ф╥╠Е╨╕И ░Е▐≤И┤▐Ф╗║Е·▀О╪▄Е╩╨Г╚▀Е°╗И ░Г╘╨И≈╢Д╦┼О╪▄И ░Г╘╨И≈╢Д╦╜Г └Е▐≤И┤▐Ф≤╞И ╬Д╩╔Г⌡╢Ф▌╔Х╖┌Е╞÷Е╬≈Е┬╟Г └О╪▄Е╬─Е╬─Е╬┬И ╬Е°╗Ф∙╟Ф█╝Д╦╜Г⌡╢Ф▌╔Д╫⌠Г▌╟О╪▄Д╫├Ф≤╞Г║╝Е╝·Е╜≤Е°╗Г²─И ░Г╘╨И≈╢О╪▄Д╩╔mnistФ┴▀Е├≥Д╫⌠Ф∙╟Ф█╝Е╨⌠Ф²╔Х╞╢Г └Х╞²О╪▄Е╟╫Г╝║Ф┴▀Е├≥Д╫⌠Г └Г╩╢Е╨╕Ф≈╤ \(28 \times 28\) Г╩╢О╪▄Д╫├Ф≤╞Е╝·И≥┘Д╦┼Е°╗Е╝·Х╥╣Х©┤Г╗▀Д╦╜О╪▄Е°╗Ф┴▀Е├≥Х©┤Г╗▀Д╦╜О╪▄Е╓╖Х└▒Е▌╩Ф·└Е╩╨Ф┴▀Е├≥Д╫⌠Ф∙╟Е╜≈Г └Г╩⌠Ф·└Ф≈╤О╪▄Ф┐ЁГ └Ф≤╞Г╛■Г■╩Е╓ И┤█О╪▄Е╪╞Ф┼≤Ф─▌Д╧┬Е├≥О╪▄Х©≥И┐╗Е┬├Ф─²Ф┐ЁХ©┤Г╗▀Е▐╞Д╩╔Е╫▓Г╨ЁЕ┬╟И ░Г╘╨И≈╢Е╩╨Ф╗║Д╦┼</p>

<p>Г■÷Ф┬░Ф╗║Е·▀Д╧÷Ф≤╞Д╦─Ф═╥Г └О╪▄Г⌡╢Ф▌╔Е╩╨Г╚▀ \(p(\mathbf{x})\) Е╬┬И ╬О╪▄Д╫├Ф≤╞Е▐╞Д╩╔И─ Х©┤Д╦─Г╩└И ░Е▐≤И┤▐Ф²╔Е╩╨Ф╗║О╪ </p>

\[p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z} = \int p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) d \mathbf{z}\]

<p>Д╫├Ф≤╞Е°╗Е╝·И≥┘Е╩╨Ф╗║Д╦╜О╪▄Д╦┼Х©╟Г └Ф╗║Е·▀Ф≈╤Е╬┬И ╬Х╝║Г╝≈Г └О╪▄Е█Ё DLVM Г └Д╦█Е▐╞Е╓└Г░├Ф─╖</p>

<p>Ф°─Е╓╖Д╪╪Г└╤Д╪╟Х╝║Е°╗ DLVM И°─Х╕│Х╖ёФ·░Г └О╪▄Ф┬√Х─┘Д╪╟Х╝║Е┤╨ \(p_{\boldsymbol{\theta}}(\mathbf{x})\)О╪▄Е├█И─ Х©┤Д╪≤Е▄√ \(p_{\boldsymbol{\theta}}(\mathbf{x})\) Ф┴╬Е┤╨ \(\theta^*\)О╪▄Д╫├Ф≤╞Г■╠Д╨▌Ф╠┌Х╖ёГ╖╞Е┬├ \(p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z}\) Г └Х©┤Г╗▀О╪▄Х©≥Д╦╙Д╦█Х┐╫Г⌡╢Ф▌╔Х╖ёФ·░Г └Е│ Е┤╨О╪▄Ф┬√Д╪╟Х╝║Е┤╨О╪▄И─ Х©┤ MLE Д╪╟Х╝║Ф²╔Д╪≤Е▄√Ф≤╞Д╦█Е▐╞Е╓└Г░├Г └О╪▄Е░▄Ф≈╤Х©·Е╦╕Г └О╪▄Г■╠Д╨▌Е░▌И╙▄Е┬├Е╦┐Ф°┴Х╝║Г╝≈Е┘╛Е╪▐О╪ </p>

\[p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})=\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{x})}\]

<p>Е⌡═Ф╜╓Е░▌И╙▄Е┬├Е╦┐Д╧÷Ф≤╞Д╦█Х┐╫Х╝║Г╝≈Г └О╪▄Е╕┌ MAP Г╜┴Ф√╧ФЁ∙Е░▄Ф═╥Д╦█Х┐╫Д╫©Г■╗О╪▄Д╦╨Д╨├Х╖ёЕ├ЁД╦█Е▐╞Е╓└Г░├Ф─╖О╪▄Г■÷Ф┬░Ф╗║Е·▀Д╦╜Ф°┴Е┤═Г╠╩Ф√╧Ф║┬О╪▄Д╦▀И²╒Д╦╩Х╕│Е┘ЁФЁ╗ Variational Autoencoders</p>

<h2 id="2-variational-autoencoders">2. Variational Autoencoders</h2>

<p>Х╝╜Г╩┐Е╝▄Ф┬░Г └ VAE О╪▄Д╩▌И ░Г╘╨И≈╢Д╦╜И┤┤Ф═╥ \(\boldsymbol{z}\)О╪▄Е┬╘Г■╗Х╖ёГ═│Е≥╗ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) Е▌╩Е╓█Е╩╨Е┤╨ \(\boldsymbol{X}\)О╪▄Е┘╤Ф╗║Е·▀Е╕┌Д╦▀Ф┴─Г╓╨</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2О╪ Generative Model Part 2О╪ A Survey on Variational Autoencoders.md/latent_encoder_graph.png" alt="latent_encoder_graph" style="zoom:30%;" /></p>

<p>Д╫├Ф≤╞Г■╠Д╨▌Е░▌И╙▄Д╦█Е▐╞Е╓└Г░├Ф─╖О╪▄Г■÷Ф┬░Ф╗║Е·▀Е╧╤Ф≈═ФЁ∙Г⌡╢Ф▌╔Ф═╧Ф█╝Ф∙╟Ф█╝И⌡├Г⌡╢Ф▌╔Ф╠┌Х╖ё \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) О╪▄Ф°┴Е┤═Г╖█Х╖ёЕ├ЁЕ┼·ФЁ∙Ф²╔Е╓└Г░├Х©≥Д╦╙И≈╝И╒≤Ц─┌VAE Д╩█Г└╤Д╩▌Ф°─Е╓╖Х╬╧И≥┘Е░▌И╙▄ \(p_{\theta}(\mathbf{x})\) Г └Ф┐ЁФЁ∙Е┤╨Е▐▒О╪▄Д╫├Ф≤╞Д╪≤Е▄√Е░▌И╙▄Г └Д╦─Д╦╙Д╦▀Г∙▄Ф²╔Д╫©Е░▌И╙▄Е▐≤Е╓╖О╪┬Х╖│Е░▌Ф√┤ ELBO И┐╗Е┬├О╪┴О╪▄Ф┐ЁФЁ∙Ф°┴Е─÷И┴╢Х┤╙Е┼╗Г╪√Г═│Е≥╗О╪┬AutoencoderО╪┴О╪▄Д╫├Ф≤╞Г■╠Д╨▌Х╬╧И≥┘Е░▌И╙▄ \(p_{\boldsymbol{\theta}}(\mathbf{x})\) Д╦█Е▐╞Е╓└Г░├Ф─╖Ц─┌VAEХ║╔Е┘╗Д╨├Х┤╙Е┼╗Г╪√Г═│Е≥╗Г └Е┴█Е█┼И┐╗Е┬├О╪▄И─ Х©┤Е│ Д╦─Д╦╙Е▐┌Ф∙╟Ф▌╗Ф√╜Ф╗║Е·▀ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Ф²╔Ф⌡©Ф█╒ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) Ф╗║Ф▀÷Г╪√Г═│Е≥╗
\(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \approx p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2О╪ Generative Model Part 2О╪ A Survey on Variational Autoencoders.md/VAE-illustration.png" alt="VAE-illustration" style="zoom:30%;" /></p>

<p>Ф╜ёЕ╕┌Д╦┼Е⌡╬Ф┴─Г╓╨О╪▄Е╕┌Ф·°Е╪╨Х╟┐ VAE Г └Х┤╙Е┼╗Г╪√Г═│Е≥╗Г └Е╠·Ф─╖О╪▄Е┬≥Ф▌╗Ф√╜Е┬├Е╦┐ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Ф·└Ф┬░Д╨├Г╪√Г═│Е≥╗О╪▄Х╖ёГ═│Е≥╗Е┬≥Ф≤╞ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)О╪▄Е╫⌠Г└╤Е╕┌Ф·°Х┐╫Е╓÷Г⌡╢Ф▌╔Ф╠┌Х╖ё \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) Ф≤╞Ф°─Е╔╫Г └О╪▄Д╫├Ф≤╞Д╦┼Х©╟Х╖ёГ═│Е≥╗Ф≤╞Ф≈═ФЁ∙Г⌡╢Ф▌╔Д╪≤Е▄√Г └О╪▄Е⌡═Ф╜╓Ф┴█И°─Х╕│Е├█Е│ Д╦─Д╦╙Г╪√Г═│Е≥╗Г └Ф▌╗Ф√╜Е┬├Е╦┐ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) О╪▄Ф°┴Д╨├Ф▌╗Ф√╜Е┬├Е╦┐Е░▌О╪▄Е╟╠Е▐╞Д╩╔Д╩▌Е┘╤Д╦╜И┤┤Ф═╥ \(z\)О╪▄Е├█И∙°Е┐▐Г └Е┬╘Г■╗Х╖ёГ═│Е≥╗ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) Ф≤═Е╟└Е⌡·Ф∙╟Ф█╝Г╘╨И≈╢Е░▌О╪▄Е┬╘Г■╗Д╨╓Е▐┴Г├╣Е▌╩Г╝≈Е┤╨ lossО╪▄Е┬╘Г■╗ SGD Е▌╩Д╪≤Е▄√Г╪√Г═│Е≥╗Е▓▄Х╖ёГ═│Е≥╗Г └Е▐┌Ф∙╟ \(\phi,\theta\)</p>

<p>Е°╗Е┘╥Д╫⌠И≈╝И╒≤Д╦╜О╪▄И─ Х©┤Д╦─Д╦╙Ф°┴Е░▒Ф≈═Г▌╞Е⌡╬Г╩⌠Ф·└Ф²╔Е╩╨Ф╗║Ф▌╗Ф√╜Е┬├Е╦┐О╪ </p>

\[q_{\phi}(\mathbf{z} \mid \mathbf{x})=q_{\phi}\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{M} \mid \mathbf{x}\right)=\prod_{j=1}^{M} q_{\phi}\left(\mathbf{z}_{j} \mid P a\left(\mathbf{z}_{j}\right), \mathbf{x}\right)\]

<p>Д╦╨Д╨├Е┘╥Д╫⌠Г └Е╩╨Ф╗║Д╦┼Х©╟И─╪Х©▒О╪▄VAE И┤┤Г■╗Д╨├Д╦╓Ф╜╔О╪▄Е█Ё \(Evidence \; Lower \; Bound\) Ф²╔Д╫°Д╦╨Д╪≤Е▄√Г⌡╝Ф═┤О╪▄Е░▄Ф≈╤Е°╗Е┘╥Д╫⌠Д╪≤Е▄√Х©┤Г╗▀Д╦╜И┤┤Г■╗Д╨├ \(Reparameterization\; Trick\) Ф²╔Х╖ёЕ├ЁФ╒╞Е╨╕Г └Ф╠┌Х╖ёИ≈╝И╒≤</p>

<h3 id="21-loss-function-evidence-lower-bound">2.1 Loss function (Evidence Lower Bound)</h3>

<p>Х╬╧И≥┘Е░▌И╙▄ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) Д╦▌ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Г▀╛Г╚▀О╪▄Е⌡═Ф╜╓И─ Х©┤И┤█Е├≥Ф┬░Ф°÷Ф°⌡О╪▄Е▐╞Д╩╔Ф▌╗Е╞╪Е┤╨ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) Г╜┴Д╩╥Д╨▌ ELBO Е┼═Д╦─Д╦╙KLФ∙ёЕ╨╕</p>

\[\begin{aligned}
\log p_{\boldsymbol{\theta}}(\mathbf{x}) 
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x})\right] 
\\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;= \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;= \underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\\(\mathrm{ELBO})}
+
\underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=D_{K L}\left(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)}

\end{aligned}\]

<p>И┌ёД╧┬Ф°─Е╓╖Е░▌И╙▄Х©▒Д╪╪Д╨▌Д╪≤Е▄√ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) (ELBO)</p>

\[\begin{aligned}
\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x}) 
&amp;= \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]
\\
 &amp;=\log p_{\boldsymbol{\theta}}(\mathbf{x})-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right) \\
&amp; \leq \log p_{\boldsymbol{\theta}}(\mathbf{x})
\end{aligned}\]

<p>Х─▄Г╛╛Д╨▄И║╧KLФ∙ёЕ╨╕О╪▄Е╬┬Е╥╖Е╕≥Г └Е░▄Ф≈╤Х║╗Х╬╬Д╨├Д╦╓Д╦╙Х╥²Г╕╩О╪ </p>

<ol>
  <li>ELBOЕ▓▄Х╬╧И≥┘Е░▌И╙▄Д╦╜И≈╢Г └Х╥²Г╕╩</li>
  <li>Ф▌╗Ф√╜Е┬├Е╦┐ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Д╦▌Е░▌И╙▄ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) Г └Х╥²Г╕╩</li>
</ol>

<p>Д╪≤Е▄√Г⌡╝Ф═┤Д╩▌Х╬╧И≥┘Е░▌И╙▄Е┬╟  \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) О╪▄Ф╞■Х╣╥Д╦█Е▐╞Е╓└Г░├Г └Х╬╧И≥┘Е░▌И╙▄О╪▄ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) Х┐╫Е╓÷И─ Х©┤И ▐Ф°╨Ф╒╞Е╨╕Д╦▀И≥█Х©⌡Х║▄Д╪≤Е▄√О╪▄Г░├Х╝╨Д╦┼Ф²╔Х╞╢Д╪≤Е▄√ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) Е▐╞Д╩╔Е░▄Ф≈╤Х╬╬Ф┬░Д╦╓Д╦╙Г⌡╝Ф═┤О╪ </p>

<ol>
  <li>Д╪≤Е▄√Х╬╧И≥┘Е┬├Е╦┐ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) Г └Е▐≤Е┬├Д╦▀Г∙▄О╪▄Д╫©Е╬≈Х╬╧И≥┘Е┬├Е╦┐Д╦█Е╟▐О╪▄Х©▒Д╪╪Г └Х╬╬Ф┬░Ф°─Е╓╖Д╪╪Г└╤Г └Ф∙┬Ф·°</li>
  <li>Д╫© \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) Г╛╛Д╨▄И║╧ \(-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)\) Е▐≤Е╓╖О╪▄Е█ЁД╫©Ф▌╗Ф√╜Е┬├Е╦┐Е▓▄Е░▌И╙▄Е┬├Е╦┐Г └Х╥²Г╕╩Е▐≤Е╟▐О╪▄Е╬≈Е┬╟Д╦─Д╦╙Ф⌡╢Е┤├Г║╝Г └Г╪√Г═│Е≥╗Е┬├Е╦┐О╪▄Д╦─Ф≈╕Х┐╫Е╓÷Е┤├Г║╝Е╩╨Ф╗║Г╪√Г═│Е≥╗Е┬├Е╦┐О╪▄Ф°┴Е┼╘Д╨▌Д╪≤Е▄√Х╖ёГ═│Е≥╗Е┬├Е╦┐ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</li>
</ol>

<h3 id="22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick">2.2 Stochastic Gradient-Based Optimization of the ELBO (Reparameterization Trick)</h3>

<h4 id="221-reparameterization-trick">2.2.1 Reparameterization Trick</h4>

<p>Г╩≥Д╦─Д╦╙ \(i.i.d\) Ф∙╟Ф█╝И⌡├ \(\mathcal{D}\)О╪▄Ф∙╟Ф█╝И⌡├Д╨╖Г■÷ \(\mathcal{L}_{\theta, \phi}(\mathcal{D})\) Г╜┴Д╩╥Д╨▌ \(\sum_{\mathbf{x} \in \mathcal{D}} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)О╪▄И┌ёД╧┬Д╦▀И²╒Г └Х╝╗Х╝╨Д╦╜О╪▄Е╟╠Е▐≤Ф┬░Х╝║Г╝≈ \(\mathcal{L}_{\theta, \phi}(\mathbf{x})\)О╪▄Х©≥Ф═╥Ф≤╞Г╜┴Д╩╥Г └О╪▄И┤┤Г■╗Ф╒╞Е╨╕ФЁ∙Е▌╩Д╪≤Е▄√ \(\sum_{\mathbf{x} \in \mathcal{D}} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)</p>

<p>Ф╒╞Е╨╕ \(\nabla_{\boldsymbol{\theta}, \phi} \mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) О╪▄Е╞╧Д╨▌ \(\theta\) Ф°┴О╪ </p>

\[\begin{aligned}
\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) &amp;=\nabla_{\boldsymbol{\theta}} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right] \\

&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right)\right] \\

&amp; = \nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right) \\

&amp;=\nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})\right)
\end{aligned}\]

<p>Д╫├Ф≤╞И≈╝И╒≤Е°╗Д╨▌О╪▄Г■╠Д╨▌Д╧▀Е┴█Е╞╧Д╨▌ \(\phi\)О╪▄Г■╠Д╨▌ \(z\) Ф≤╞ \(\phi\) Г └Е┤╫Ф∙╟О╪▄Х©≥Ф═╥Ф╒╞Е╨╕И ╬Д╩╔Х╖ёФ·░Г └Г⌡╢Ф▌╔Х╝║Г╝≈</p>

\[\begin{aligned}
\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x}) &amp;=\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right] \\
&amp; \neq \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\nabla_{\phi}\left(\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right)\right]
\end{aligned}\]

<p>VAE Ф▐░Д╬⌡Г └Х╖ёЕ├ЁЕ┼·ФЁ∙Ф≤╞Е▌╩Ф·└И─═Д╦─Д╦╙ \(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\) Ф≈═Е│▐Д╪╟Х╝║ \(\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\) О╪▄Е█ЁЕ▐╕Д╦─Д╦╙Ф┼─Е╥╖ \(Reparameterization \; Trick\)</p>

<p>\(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\) Ф≤╞Г■╠Д╨▌Х╝║Г╝≈ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Г └Ф°÷Ф°⌡О╪▄Е├█Ф╜╓Е÷╨Г║─Д╦┼Г╝≈ \(\phi\) Г └Е│▐Е╞╪Ф∙╟Е╟╠Е╞╪Х┤╢И ╬Д╩╔Х╖ёФ·░Г └Х╝║Г╝≈О╪▄Х©≥Ф═╥Е╟╠Ф≈═ФЁ∙Ф┴╖Х║▄Ф╒╞Е╨╕Д╦▀И≥█Г╝≈ФЁ∙О╪▄Д╨▌Ф≤╞Д╫°Х─┘Е°╗ \(Reparameterization \; Trick\) Д╦╜Х╝╬Х╝║Д╨├Д╦─Д╦╙Ф≈═Е│▐Г╩÷Х╝║И┤▐Ф²╔Д╪╟Х╝║ \(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)О╪ </p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2О╪ Generative Model Part 2О╪ A Survey on Variational Autoencoders.md/Reparameterization-Trick.png" alt="Reparameterization-Trick" style="zoom:40%;" /></p>

<p>Е╕┌Д╦┼Е⌡╬Ф┴─Г╓╨О╪▄Ф·└И─═Д╦─Д╦╙Е≥╙Её╟Е▐≤И┤▐ \(\epsilon\) Ф°┴Е┬├Е╦┐ \(p(\boldsymbol{\epsilon})\)О╪▄Е╝ Д╧┴Ф√╟Г └Ф≤═Е╟└Е┘ЁГЁ╩О╪ \(\mathbf{z}=\mathbf{g}(\boldsymbol{\epsilon}, \boldsymbol{\phi}, \mathbf{x})\) Ф╩║Х╤ЁО╪▄Е┘╤Д╦╜ \(g(.)\) Ф°╙Г÷╔О╪▄Д╫├Ф≤╞ \(\epsilon\) Г⌡╦Е╞╧Г └Ф╞■Х╬┐Г╝─Е█∙</p>

\[\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[f(\mathbf{z})]=\mathbb{E}_{p(\boldsymbol{\epsilon})}[f(\mathbf{z})]\]

<p>Х©≥Ф═╥Е╟╠Е▐╞Д╩╔Ф┼┼ \(z\) Г └И ▐Ф°╨Ф─╖Х╫╛Г╖╩Е┬╟ \(\epsilon\) Д╦┼О╪▄Е┘┬Е°╗Ф┼╫Х╠║Д╦─Д╨⌡Г └И≈╝И╒≤Х╝╬Е╝ Д╦▀Г═■Г╘╤ \(Reparameterization \; Trick\)О╪▄Е°╗Ф╜╓Е÷╨Г║─Д╦┼Е╞╧ \(\phi\) Ф╠┌Е╞╪О╪▄Е▐╞Д╩╔Ф°┴Д╦─Д╦╙ \(\nabla_{\boldsymbol{\phi}} f(\mathbf{z})\) Г └Д╪╟Х╝║О╪ </p>

\[\begin{aligned}
\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[f(\mathbf{z})] &amp;=\nabla_{\phi} \mathbb{E}_{p(\boldsymbol{\epsilon})}[f(\mathbf{z})] \\
&amp;=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\phi}} f(\mathbf{z})\right] \\
&amp; \simeq \nabla_{\boldsymbol{\phi}} f(\mathbf{z})
\end{aligned}\]

<p>Ф⌡╢Х©⌡Д╦─Ф╜╔О╪▄Д╩ёЕ┘╔Е┘╥Д╫⌠Г └Е┤╫Ф∙╟ \(f(z) = \log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\)О╪▄Ф·└И─═Д╦─Д╦╙Х▓≥Г┴╧Е█║Ф╢⌡Д╪╟Х╝║
\(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x})\)О╪ </p>

\[\begin{aligned}
\boldsymbol{\epsilon} &amp; \sim p(\boldsymbol{\epsilon})
\\
\mathbf{z} &amp;=\mathbf{g}(\boldsymbol{\phi}, \mathbf{x}, \boldsymbol{\epsilon}) 
\\
\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) &amp;=\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})
\end{aligned}\]

<p>Е▐╞Д╩╔Х╞│Ф≤▌ \(\nabla_{\boldsymbol{\theta}, \phi} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x})\) Ф≤╞ \(\nabla_{\boldsymbol{\theta}, \phi} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\) Ф≈═Е│▐Д╪╟Х╝║О╪ </p>

\[\begin{aligned}
\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\theta}, \phi} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\right] &amp;=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right)\right] \\
&amp;=\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]\right) \\
&amp;=\nabla_{\boldsymbol{\theta}, \phi} \mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})
\end{aligned}\]

<p>Е°╗Е╝·И≥┘Х╝║Г╝≈Д╦╜О╪▄Г╠╩Д╪╪SGDГ └Ф─²Ф┐ЁД╦─Ф═╥О╪▄Е┬╘Г■╗Е╓ Ф╛║Ф╞■Х╬┐Б─°Ф╗║ГЁ┼Б─²Г └Х╝║Г╝≈Ф²╔Ф⌡©Ф█╒Е█∙Ф╛║Б─°Г╡╬Е┤├Б─²Г └Х╝║Г╝≈О╪▄Е█ЁГ⌡╢Ф▌╔Х╝║Г╝≈О╪ 
\(\nabla_{\boldsymbol{\theta, \phi}}\log p_{\theta}(\mathbf{x}  \mid \mathbf{z} )+\log p_{\phi}(\mathbf{z} )-\log q(\mathbf{z}  \mid \mathbf{x} )
\quad
\mathbf{z}  \text{ О╫· } q(\mathbf{z}  \mid \mathbf{x} )\)
Е╫⌠Г└╤Х©≥Ф═╥Х╞╢Е▐╞Х┐╫Х©≤Ф≤╞Ф°┴Д╦─Д╨⌡Ф┼╫Х╠║О╪▄Д╦▀И²╒Д╪ Е┘╥Д╫⌠Г └Д╦╬Д╦─Д╦╙Е┘╥Д╫⌠Г └И ░Г╘╨И≈╢ \(z\) Е▓▄Г⌡╦Е╨■Г └ \(\epsilon\) Г └Д╬▀Е╜░</p>

<h4 id="222-computation-of-inference-distribution">2.2.2 Computation of Inference Distribution</h4>

<p>Е°╗Х╝║Г╝≈ \(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) =\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\) Г └Х©┤Г╗▀Д╦╜О╪▄И°─Х╕│Х╝║Г╝≈Г╪√Г═│Е≥╗/Ф▌╗Ф√╜Е┬├Е╦┐ \(\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\)</p>

\[\begin{aligned}
\boldsymbol{\epsilon} &amp; \sim p(\boldsymbol{\epsilon})
\\
\mathbf{z} &amp;=\mathbf{g}(\boldsymbol{\phi}, \mathbf{x}, \boldsymbol{\epsilon}) 
\\
p(\boldsymbol{\epsilon}) &amp;= q_{\phi}(\mathbf{z} \mid \mathbf{x})\left|\operatorname{det}\left(\frac{\partial \mathbf{z}}{\partial \boldsymbol{\epsilon}}\right)\right|

\end{aligned}\]

<p>Е⌡═Ф╜╓Е▐╞Д╩╔Г°▀Е┤╨О╪▄И─┴Ф▀╘Д╦─Д╦╙Е░┬И─┌Г └Е▐╞И─├Е▐≤Е▄√ \(g(\epsilon)\) Д╪ Д╫©Е╬≈Х╝║Г╝≈Г╝─Е▄√</p>

\[\log q_{\phi}(\mathbf{z} \mid \mathbf{x})=\log p(\boldsymbol{\epsilon})-\log \left|\operatorname{det}\left(\frac{\partial \mathbf{z}}{\partial \boldsymbol{\epsilon}}\right)\right|\]

<p>Д╦▀И²╒Д╦╬Д╦─Д╦╙Е┘╥Д╫⌠Х╝║Г╝≈Д╦╜Ф·└И─═Г╪√Г═│Е≥╗Е┬├Е╦┐ \(\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\) Е▓▄ \(Reparameterization \; Trick\) Г └Д╬▀Е╜░О╪ </p>

<p>Х╝╬И ░Г╘╨И≈╢ \(\mathbf{z}\) Ф°█Д╩▌Ф╜ёФ─│Е┬├Е╦┐О╪▄Х─▄Ф²║Д╩╤Е┬├Е╦┐ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Г └Е▐┌Ф∙╟И─ Х©┤Г╪√Г═│Е≥╗Х╝║Г╝≈Е╬≈Е┬╟О╪ </p>

\[\begin{aligned}
q_{\phi}(\mathbf{z} \mid \mathbf{x})  &amp;= \mathcal{N}(\mathbf{z} ; \boldsymbol{\mu}, \operatorname{diag}\left(\sigma^{2}\right))
\\
(\boldsymbol{\mu}, \log \boldsymbol{\sigma}) &amp;=\text { EncoderNeuralNet}_{\boldsymbol{\phi}}(\mathbf{x}) 
\\
q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) &amp;=\prod_{i} q_{\boldsymbol{\phi}}\left(z_{i} \mid \mathbf{x}\right)=\prod_{i} \mathcal{N}\left(z_{i} ; \mu_{i}, \sigma_{i}^{2}\right)
\end{aligned}\]

<p>Х©≥Ф═╥Г⌡╢Ф▌╔Г╝≈Д╦▀Е▌╩Г └Х╞²О╪▄Д╧▀Е░▌Ф≈═ФЁ∙Е▐█Е░▒Д╪═Ф▓╜О╪▄И┤┤Г■╗ \(Reparameterization \; Trick\)О╪ </p>

\[\begin{array}{l}
\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}) \\
\mathbf{z}=\mu+\sigma \odot \epsilon
\end{array}\]

<p>Х©⌡Д╦─Ф╜╔Е╟╠Е▐╞Д╩╔Х╝║Г╝≈Г╪√Г═│Е≥╗Е┬├Е╦┐ \(\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Д╦╨О╪ 
\(\begin{aligned}
\log q_{\phi}(\mathbf{z} \mid \mathbf{x}) &amp;=\log p(\boldsymbol{\epsilon})-\log d_{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon}) \\
&amp;=\sum_{i} \log \mathcal{N}\left(\epsilon_{i} ; 0,1\right)-\log \sigma_{i}
\end{aligned}\)</p>

<h3 id="23-estimation-of-the-marginal-likelihood">2.3 Estimation of the Marginal Likelihood</h3>

<p>Х╬╧И≥┘Д╪╪Г└╤Е▐╞Д╩╔Е┬╘Г■╗Г╪√Г═│Е≥╗Е┬├Е╦┐Е▌╩Ф·└И─═О╪ </p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x})=\log \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) / q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]\]

<p>Х─▄Е°╗Ф∙╟Е─╪Д╦┼О╪▄Д╦┼Е╪▐Е▐╞Д╩╔Е┬╘Г■╗Х▓≥Г┴╧Е█║Ф╢⌡ФЁ∙Х©⌡Х║▄Д╪╟Х╝║О╪ </p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x}) \approx \log \frac{1}{L} \sum_{l=1}^{L} p_{\boldsymbol{\theta}}\left(\mathbf{x}, \mathbf{z}^{(l)}\right) / q_{\boldsymbol{\phi}}\left(\mathbf{z}^{(l)} \mid \mathbf{x}\right)\]

<p>Е╫⌠ \(L=1\) Ф≈╤О╪▄Х©≥Д╦╙Е╪▐Е╜░Е╟╠Ф≤╞ \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\)</p>

<h3 id="25-summary-of-training">2.5 Summary of Training</h3>

<p>VAEО╪▄Е▓▄GANД╦─Ф═╥О╪▄Ф°─Е░▌И─ Х©┤Д╩▌И ░Г╘╨И≈╢И┤┤Ф═╥О╪▄Г└╤Е░▌Г■╗Х╖ёГ═│Е≥╗Ф≤═Е╟└Е┬╟Ф∙╟Ф█╝Г╘╨И≈╢О╪▄Г■÷Ф┬░Ф∙╟Ф█╝О╪▄Д╫├Ф≤╞Ф╞■Х╣╥GANИ─ Х©┤Е▌╩Х©╜Д╩ёХ╝╜Г╩┐Г■÷Ф┬░Е≥╗Е▓▄Е┬╓Е┬╚Е≥╗Г └Г╩⌠Ф·└О╪▄Ф°─Е░▌Ф┼┼Г■÷Ф┬░Е≥╗Е╫⌠Д╫°Ф≤═Е╟└О╪▄VAEФ⌡╢Е┼═Е╪╨Х╟┐Е╞╧Д╨▌Ф∙╟Ф█╝Е┬├Е╦┐Г └Г░├Х╖ё</p>

<ul>
  <li>
    <p>VAE Е│┤Х╝╬И ░Г╘╨И≈╢ \(\mathbf{z}\) Г └Г╪√Г═│Е≥╗Е┬├Е╦┐ \(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\) Ф°█Д╩▌Д╦─Д╦╙И╚≤Ф√╞Е┬├Е╦┐О╪┬Е╪╨Х╟┐Ф╗║Е·▀Г └Х║╗Х╬╬Х┐╫Е┼⌡И─┴Ф▀╘Е█┼Д╦┴Х╖▓Д╫°Д╦╨И╚≤Ф√╞Е┬├Е╦┐Ф√╧Е╥╝О╪▄Е▐█Д╧▀И─┴Ф▀╘Е╞╧Х╖▓Г÷╘И≤╣Д╫°Д╦╨Ф√╧Е╥╝О╪┴О╪▄Е┬╘Г■╗Г╪√Г═│Е≥╗Ф┼┼ \(\mathbf{x}\) Д╩▌Ф∙╟Ф█╝Г╘╨И≈╢Ф≤═Е╟└Е┬╟И ░Г╘╨И≈╢О╪▄Х╬⌠Е┤╨Е┬├Е╦┐Г └ \(\mu,\sigma\)</p>
  </li>
  <li>
    <p>И─ Х©┤ \(Reparameterization \;trick\) Ф┼┼ \(\mathbf{z}\) Г └И ▐Ф°╨Ф─╖Х╫╛Е┬╟ \(\epsilon\) Д╦┼О╪▄И┤┤Ф═╥ \(\mathbf{z}\)</p>
  </li>
  <li>
    <p>\(\mathbf{z}\) Г╩▐Х©┤Х╖ёГ═│Е≥╗О╪▄Е┬╘Г■╗Д╨╓Е▐┴Г├╣Х╝║Г╝≈Е┤╨Е░▌И╙▄Е┬├Е╦┐ \(p(\mathbf{x}  \mid \mathbf{z} )\)О╪▄Ф°─Е░▌Х╝║Г╝≈Д╨╖Г■÷Ф█÷Е╓╠Х©⌡Х║▄Е▐█Е░▒Д╪═Ф▓╜
\(\mathcal{L}_{\theta, \phi}(\mathcal{\mathbf{x} }) = \log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) +\log p(\mathbf{z}) -\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \quad (L=1 )\)</p>
  </li>
</ul>

<p>Д╦┼Е╪▐Д╦╜О╪▄И²·Е╦╦Е─╪Е╬≈ФЁ╗Ф└▐Г └Д╦─Г┌╧Ф≤╞О╪▄Е░▌Д╦╓И║╧Е┘╤Е╝·Д╦█Е▌╩И≥░Е╝ Е°╗ \(L=1\) Г └Ф²║Д╩╤Д╦▀Г └Х╞²Г╜┴Д╩╥Д╨▌ \(-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\| p_{\theta}(\mathbf{z})\right)\) О╪▄Х©≥Ф═╥Е°╗Х╝╜Г╩┐Х©┤Г╗▀Д╦╜О╪▄И ░Г╘╨И≈╢Г └Е┘┬И╙▄Е┬├Е╦┐ \(p_{\theta}(\mathbf{z})\) Е▓▄Х╖ёГ═│Е≥╗Г■╗Ф²╔Х╝╜Г╩┐Х┤╙Е╥╠Г └Е┬├Е╦┐ \(q_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) Д╦█Д╪ Е│▐Г╕╩Е╬┬Х©°О╪▄Х©≥Ф═╥Е╟╠Е▐╞Д╩╔Е°╗Ф≈╤Е─≥Х©⌡Х║▄Г■÷Ф┬░Ф≈╤О╪▄И ░Г╘╨И≈╢Д╦╜И┤┤Ф═╥Г └ \(\mathbf{z}\) Д╪ Г╕╩Д╧▀Е┴█Х╖ёГ═│Е≥╗Е╜╕Д╧═Х©┤Г └Ф═╥Ф°╛Г └Ф∙╟Ф█╝Ф╣│Е╫╒Е╓╙Х©°</p>

<p>Д╦▀И²╒Е─÷Г■╗ Tensorflow.org Е╪─Ф╨░Г └ <a href="https://www.tensorflow.org/tutorials/generative/cvae">Convolutional Variational Autoencoder</a> Г └Е╝·Г▌╟</p>

<h4 id="251-data-preposscessing">2.5.1 Data Preposscessing</h4>

<p>Ф∙╟Ф█╝И╒└Е╓└Г░├Д╦╜О╪▄Е╞╧Е⌡╬Е┐▐Х©⌡Х║▄Г│╟Е╨╕-&gt;И╩▒Г≥╫Е╓└Г░├О╪▄Х©≥Ф═╥Ф°─Е░▌Х╬⌠Е┤╨Е┬╘Г■╗Д╪╞Е┼╙Е┬╘Д╨▄И║╧Е┬├Е╦┐Х©⌡Х║▄Х╬⌠Е┤╨Е█ЁЕ▐╞О╪ </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_images</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
  <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mf">255.</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">images</span> <span class="o">&gt;</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>

<span class="n">train_images</span> <span class="o">=</span> <span class="n">preprocess_images</span><span class="p">(</span><span class="n">train_images</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">preprocess_images</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>

</code></pre></div></div>

<h4 id="252-network-architecture">2.5.2 Network Architecture</h4>

<p>CVAEГ╫▒Г╩°Г╩⌠Ф·└Ф≤╞Д╦─Д╦╙Д╦─Е╞╧Х─╕Е░┬Г └Х╖ёГ═│Е≥╗Е▓▄Г╪√Г═│Е≥╗О╪▄Х╖ёГ═│Е≥╗Г■╠Д╦╓Е╠┌Е█╥Ф°╨Е╠┌Е▓▄Д╦─Е╠┌Е┘╗Х│■Ф▌╔Е╠┌Ф·└Ф┬░О╪▄Х╖ёГ═│Е≥╗Ф┼┼Ф∙╟Ф█╝Ф≤═Е╟└Е┬╟И ░Г╘╨И≈╢Д╦┼О╪┬Х╬⌠Е┤╨И ░Г╘╨И≈╢Г └Е▐┌Ф∙╟О╪┴О╪▄Х©≥И┤▄Е│┤Х╝╬Д╦─Д╦╙И╚≤Ф√╞Е┬├Е╦┐Д╫°Д╦╨И ░Г╘╨И≈╢Г └Е┬├Е╦┐О╪▄Д╦╨Д╨├Е╝·Г▌╟ \(Reparameterization \; Trick\) Е┬≥Г╩≥Е┤╨Д╨├Е▐╕Д╦─Д╦╙Ф°█Д╩▌Ф═┤Е┤├Ф╜ёФ─│Е┬├Е╦┐Г └Е▐┌Ф∙╟ \(\epsilon\)О╪ </p>

\[z=\mu+\sigma \odot \epsilon\]

<p>Г└╤Е░▌Х╖ёГ═│Е≥╗Ф≤╞Г╪√Г═│Е≥╗Г └И∙°Е┐▐О╪▄И∙°Е┐▐Г └Е┘╗Х│■Ф▌╔Е╠┌Е░▌Ф▌╔Д╨├Д╦─Е╠┌Е▐█Е█╥Г╖╞Е╠┌</p>

<p>Е┬╘Г■╗Х╖ёГ═│Е≥╗Е▓▄Г╪√Г═│Е≥╗О╪▄Е▐╞Д╩╔Х©⌡Д╦─Ф╜╔Е├≥Е┤╨ encode(.), decode(.), sample(.),, reparameterize(.) Е╧╤Е╟│Хё┘Е┬╟ CVAE Г╠╩Д╦╜</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CVAE</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="s">"""Convolutional variational autoencoder."""</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CVAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="c1"># No activation
</span>            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">+</span> <span class="n">latent_dim</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
            <span class="c1"># No activation
</span>            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">filters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>

  <span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">apply_sigmoid</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span>

  <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">mean</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logvar</span> <span class="o">*</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span>

  <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">apply_sigmoid</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_sigmoid</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">probs</span>
    <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></div>

<h4 id="253-loss-function">2.5.3 Loss Function</h4>

<p>Г└╤Е░▌Х╝║Г╝≈Ф█÷Е╓╠Е┤╫Ф∙╟Г └Х©┤Г╗▀Д╦╜О╪▄Ф√╧Е╥╝Е▐√Г └Ф≤╞ \(log var\) И≤╡Ф╜╒Ф∙╟Е─╪Д╦┼Г └И≈╝И╒≤О╪▄Е░▌И╙▄Е┬├Е╦┐Е┬╘Г■╗Д╨╓Е▐┴Г├╣Х╝║Г╝≈Е█ЁЕ▐╞О╪▄Д╦╨Д╨├Е┼═И─÷Х╝║Г╝≈О╪▄Е▐╙Д╦─Ф╛║И┤┤Ф═╥ \(z\) О╪ </p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) = - H(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}), p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}))\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
  <span class="n">x_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="n">cross_ent</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">x_logit</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
  <span class="n">logpx_z</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">cross_ent</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
  <span class="n">logpz</span> <span class="o">=</span> <span class="n">log_normal_pdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
  <span class="n">logqz_x</span> <span class="o">=</span> <span class="n">log_normal_pdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">logpx_z</span> <span class="o">+</span> <span class="n">logpz</span> <span class="o">-</span> <span class="n">logqz_x</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="24-challenges">2.4 Challenges</h3>

<p>VAEО╪▄Е▓▄GANД╦─Ф═╥О╪▄Ф°─Е░▌И─ Х©┤Д╩▌И ░Г╘╨И≈╢И┤┤Ф═╥О╪▄Г└╤Е░▌Г■╗Х╖ёГ═│Е≥╗Ф≤═Е╟└Е┬╟Ф∙╟Ф█╝Г╘╨И≈╢О╪▄Г■÷Ф┬░Ф∙╟Ф█╝О╪▄Д╫├Ф≤╞Е▓▄GANД╦█Д╦─Ф═╥Г └Ф≤╞О╪▄GANХ╝╜Г╩┐Г └Ф≤╞Е▐╞Х┐╫Ф─╖О╪┬probabilityО╪┴О╪▄Е█ЁЕ┬╘Г■╗Е┬╓Е┬╚Е≥╗Е▌╩Е┬╓Ф√╜Г■÷Ф┬░Ф∙╟Ф█╝Ф≤╞Г°÷Е╝·Ф∙╟Ф█╝Г └Е▐╞Х┐╫Ф─╖О╪▄VAEД╩█Г└╤Д╪ Е▌╩Х╝╜Г╩┐Ф∙╟Ф█╝Г └Е┬├Е╦┐О╪┬distributionО╪┴О╪▄И┌ёД╧┬Х©≥Д╦─Г┌╧Д╧÷Г╩≥VAEГ └Х╝╜Г╩┐Е╦╕Ф²╔Г └Е⌡╟И ╬</p>

<h4 id="241-optimization-issues">2.4.1 Optimization issues</h4>

<p>Ф∙╢Д╦╙Х╝╜Г╩┐Х©┤Г╗▀Д╦╜О╪▄VAE И┐╫Х╕│Е▌╩Г╝≈Ф∙╟Ф█╝Г └Е░▌И╙▄Е┬├Е╦┐ \(p(x \mid z)\) Ф²╔Х©⌡Х║▄Д╪≤Е▄√О╪▄Д╫├Ф≤╞Д╦─Е╪─Е╖▀Х╝╜Г╩┐Ф≈╤О╪▄Х╖ёГ═│Е≥╗Г └Ф∙┬Ф·°Д╪ Е╬┬Е╥╝О╪▄Ф≈═ФЁ∙Ф┼┼И ░Г╘╨И≈╢Д╦╜Г └И┤┤Ф═╥ \(z\) Ф≤═Е╟└Е┬╟Г°÷Е╝·Г └Ф∙╟Ф█╝Ф╣│Е╫╒Д╦┼О╪▄И┌ёД╧┬Ф╜╓Ф≈╤Х╝║Г╝≈Е┤╨Г └ \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\) Д╦╜ \(\log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) Е╬┬Е╟▐О╪▄Д╦■Д╪≤Е▄√Е╬┬И ╬О╪▄Д╦╨Д╨├Д╫© \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\) Е▐≤Е╓╖</p>

\[\mathcal{L}_{\theta, \phi}(\mathcal{\mathbf{x} }) = \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[\log p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) +\log p(\mathbf{z}) -\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})]\]

<p>Е░▌Е█┼И┐╗Е┬├ \(\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[\log p(\mathbf{z}) /q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})] =  -D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\| p_{\theta}(\mathbf{z})\right)\) Е╟╠Д╪ Е▐≤Е╓╖О╪▄Е▐┬Г■╠KLФ∙ёЕ╨╕Г └И²·Х╢÷Ф─╖О╪▄Ф╜╓Ф≈╤Ф╗║Е·▀Е╟╠Д╪ Е┬╟Х╬╬Д╦─Д╦╙Ф≈═Ф└▐Д╧┴Г └Е²┤Х║║Г┌╧ \(\log p(\mathbf{z}) \approx \log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\)О╪▄Ф╜╓Ф≈╤Г╪√Г═│Е≥╗ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) Ф╡║Ф°┴Ф─╖Х┐╫Е▌╩Е│ Ф°┴Ф└▐Д╧┴Г └Г╪√Г═│О╪▄Г╩╖Х─▄Х╖ёГ═│Е≥╗Е╬┬И ╬Ф┼┼И ░Е▐≤И┤▐  \(\mathbf{z}\) Е╓█Е╩╨Е⌡·Ф∙╟Ф█╝Ф╣│Е╫╒Д╦┼Е╞╧Е╨■Г └ \(\mathbf{x}\)</p>

<p>Х╖ёЕ├ЁФ√╧ФЁ∙Ф°┴Е°╗Х╝╜Г╩┐Х©┤Г╗▀Д╦╜О╪▄И╕√Е┘┬Е├╩Г╩⌠О╪┬Е┴█И²╒Д╧≤Д╦─Д╦╙0О╫·1Д╧▀Е┴█Г └Ф▌╖Е┬╤ГЁ╩Ф∙╟О╪┴KLФ∙ёЕ╨╕Х©≥Д╦─Ф╜ёЕ┬≥И║╧О╪▄Е°╗Х╝╜Г╩┐Х©┤Г╗▀Д╦╜И─░Ф╜╔Ф┼┼KLФ∙ёЕ╨╕Х╖ёЕ├╩О╪┬Д╫├Ф≤╞Х╖ёЕ├╩Г └И─÷Е╨╕И─┴Ф▀╘Е╟╠Е╬┬Ф°┴Г┌╪Д╦╧Г └Е▒ЁИ│⌠О╪▄Е╫⌠Х╖ёЕ├╩И─÷Е╨╕Е╓╙Е©╚Ф≈╤О╪▄Д╪ Х░╫Е┬╟Д╦┼Х©╟Г └Ф≈═Ф└▐Д╧┴Е²┤Х║║Д╦╜О╪▄Е╫⌠Х╖ёЕ├╩Г └И─÷Е╨╕Ф┘╒Д╨├Г └Х╞²О╪▄И┌ёД╧┬Д╪ Ф╣╙Х╢╧Ф≈╤И≈╢О╪▄Е╧╤Д╦■Х╖ёГ═│Е≥╗Г║╝Е╝·Ф≈═ФЁ∙Е╬≈Е┬╟Ф°┴Ф└▐Д╧┴Г └Х╝╜Г╩┐О╪┴</p>

<p>4,5,8(2)(3),9,10(2),13,14</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Diederik P. Kingma and Max Welling (2019), Б─°An Introduction to Variational AutoencodersБ─², Foundations and TrendsR in Machine Learning</li>
  <li><a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kingma%2C+D+P">Diederik P Kingma</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Welling%2C+M">Max Welling</a> (2013), Б─°Auto-Encoding Variational BayesБ─² (2013)</li>
  <li><a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Doersch%2C+C">Carl Doersch</a> (2016) Б─°Tutorial on Variational AutoencodersБ─²</li>
  <li>Tensorflow Author  <a href="https://www.tensorflow.org/tutorials/generative/cvae">Convolutional Variational Autoencoder</a> Tensorflow Author</li>
</ol>

:ET