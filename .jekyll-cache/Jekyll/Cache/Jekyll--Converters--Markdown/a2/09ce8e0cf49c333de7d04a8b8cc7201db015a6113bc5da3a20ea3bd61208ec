I"c<ul class="table-of-content" id="markdown-toc">
  <li><a href="#generative-model-part-2a-survey-on-variational-autoencoders" id="markdown-toc-generative-model-part-2a-survey-on-variational-autoencoders">Generative Model Part 2ï¼šA Survey on Variational Autoencoders</a>    <ul>
      <li><a href="#1--introduction" id="markdown-toc-1--introduction">1.  Introduction</a>        <ul>
          <li><a href="#11--probabilistic-models-and-variational-inference" id="markdown-toc-11--probabilistic-models-and-variational-inference">1.1  Probabilistic Models and Variational Inference</a></li>
          <li><a href="#12--parameterizing-conditional-distributions-with-neural-networks" id="markdown-toc-12--parameterizing-conditional-distributions-with-neural-networks">1.2  Parameterizing Conditional Distributions with Neural Networks</a></li>
          <li><a href="#13--directed-graphical-models-and-neural-networks" id="markdown-toc-13--directed-graphical-models-and-neural-networks">1.3  Directed Graphical Models and Neural Networks</a></li>
          <li><a href="#14--learning-in-fully-observed-models-with-neural-nets" id="markdown-toc-14--learning-in-fully-observed-models-with-neural-nets">1.4  Learning in Fully Observed Models with Neural Nets</a>            <ul>
              <li><a href="#141-dataset" id="markdown-toc-141-dataset">1.4.1 Dataset</a></li>
              <li><a href="#142-maximum-likelihood-and-minibatch-sgd" id="markdown-toc-142-maximum-likelihood-and-minibatch-sgd">1.4.2 Maximum Likelihood and Minibatch SGD</a></li>
            </ul>
          </li>
          <li><a href="#15-intractabilities" id="markdown-toc-15-intractabilities">1.5 Intractabilities</a></li>
        </ul>
      </li>
      <li><a href="#2-variational-autoencoders" id="markdown-toc-2-variational-autoencoders">2. Variational Autoencoders</a>        <ul>
          <li><a href="#21-loss-function-evidence-lower-bound" id="markdown-toc-21-loss-function-evidence-lower-bound">2.1 Loss function (Evidence Lower Bound)</a></li>
          <li><a href="#22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick" id="markdown-toc-22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick">2.2 Stochastic Gradient-Based Optimization of the ELBO (Reparameterization Trick)</a>            <ul>
              <li><a href="#221-reparameterization-trick" id="markdown-toc-221-reparameterization-trick">2.2.1 Reparameterization Trick</a></li>
              <li><a href="#222-computation-of-inference-distribution" id="markdown-toc-222-computation-of-inference-distribution">2.2.2 Computation of Inference Distribution</a></li>
            </ul>
          </li>
          <li><a href="#23-estimation-of-the-marginal-likelihood" id="markdown-toc-23-estimation-of-the-marginal-likelihood">2.3 Estimation of the Marginal Likelihood</a></li>
          <li><a href="#24-challenges" id="markdown-toc-24-challenges">2.4 Challenges</a>            <ul>
              <li><a href="#241-optimization-issues" id="markdown-toc-241-optimization-issues">2.4.1 Optimization issues</a></li>
            </ul>
          </li>
          <li><a href="#25-summary-of-training--generation" id="markdown-toc-25-summary-of-training--generation">2.5 Summary of Training &amp; Generation</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="generative-model-part-2a-survey-on-variational-autoencoders">Generative Model Part 2ï¼šA Survey on Variational Autoencoders</h1>

<p>â€œVAE marrys graphical models and deep learningâ€ â€”Diederik P. Kingma</p>

<p>æ­£å¦‚ä½œè€…åœ¨ã€ŠAn Introduction to Variational Autoencodersã€‹æ‰€è¨€ï¼ŒVAEç»“åˆäº†æ¦‚ç‡æ¨¡å‹ï¼Œå›¾æ¨¡å‹ï¼Œç¥ç»ç½‘ç»œã€‚</p>

<h2 id="1--introduction">1.  Introduction</h2>

<h3 id="11--probabilistic-models-and-variational-inference">1.1  Probabilistic Models and Variational Inference</h3>

<p>æ¦‚ç‡æ¨¡å‹ä¸­ï¼Œ\(\mathbf{X}\) ä»£è¡¨æ‰€æœ‰è§‚å¯Ÿæ•°æ®çš„é›†åˆï¼Œä¹Ÿæ˜¯éœ€è¦è”åˆåˆ†å¸ƒå»ºæ¨¡çš„å¯¹è±¡ï¼Œéœ€è¦å»é€¼è¿‘çœŸå®çš„åˆ†å¸ƒ \(p^{*}(\mathbf{x})\) ï¼Œåˆ©ç”¨ä¸€ä¸ªç”± \(\boldsymbol{\theta}\) æ§åˆ¶çš„æ¨¡å‹ï¼š</p>

\[\mathbf{x} \sim p_{\boldsymbol{\theta}}(\mathbf{x})\]

<p>è€Œæ¦‚ç‡æ¨¡å‹ä¸­ï¼Œå­¦ä¹ çš„æœ¬è´¨æ˜¯ï¼Œå¯»æ‰¾æœ€é€‚åˆçš„å‚æ•° \(\boldsymbol{\theta}\) æ¥é€¼è¿‘çœŸå®åˆ†å¸ƒ  \(p^{*}(\mathbf{x})\)</p>

\[p_{\boldsymbol{\theta}}(\mathbf{x}) \approx p^{*}(\mathbf{x})\]

<p>å› æ­¤ï¼Œå¸Œæœ› \(p^{*}(\mathbf{x})\) è¶³å¤Ÿçµæ´»ä»¥é€¼è¿‘ä¸€ä¸ªè¶³å¤Ÿç²¾ç¡®çš„æ¨¡å‹</p>

<p>ä¸Šè¿°æ¨¡å‹è¢«ç§°ä¸ºéæ¡ä»¶æ¦‚ç‡æ¨¡å‹ï¼Œä¸æ­¤ç›¸å¯¹åº”çš„ï¼Œè¿˜æœ‰ä¸€ç§æ¨¡å‹è¢«ç§°ä¸ºæ¡ä»¶æ¦‚ç‡æ¨¡å‹ï¼š</p>

\[p_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x}) \approx p^{*}(\mathbf{y} \mid \mathbf{x})\]

<p>å¸¸è¢«ç”¨äºå›å½’ï¼Œåˆ†ç±»é—®é¢˜ï¼Œå°½ç®¡æ¡ä»¶æ¦‚ç‡æ¨¡å‹ä¸éæ¡ä»¶æ¦‚ç‡æ¨¡å‹ä¼šåœ¨ç†è®ºä¸Šï¼Œæ˜¯ä¸¤ç§æ¦‚ç‡æ¨¡å‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œéæ¡ä»¶æ¦‚ç‡æ¨¡å‹åœ¨å®è·µä¸Šï¼Œæ˜¯ç”±è§‚å¯Ÿæ•°æ®é›† \(\mathbf{X}\) å»å»ºç«‹ \(p_{\boldsymbol{\theta}}(\mathbf{x})\) ï¼Œå› æ­¤ä¸æ¡ä»¶æ¦‚ç‡æ¨¡å‹æ˜¯ç­‰ä»·çš„ï¼Œåœ¨åé¢ä¼šè¯¦ç»†å™è¿°ã€‚</p>

<h3 id="12--parameterizing-conditional-distributions-with-neural-networks">1.2  Parameterizing Conditional Distributions with Neural Networks</h3>

<p>å°½ç®¡æå‡ºäº†ç†è®ºä¸Šçš„æ¨¡å‹ï¼Œä½†æ˜¯éœ€è¦å…·ä½“çš„æ¨¡å‹å»è¿›è¡Œè®¡ç®—ï¼Œæ¦‚ç‡æ¨¡å‹çš„å»ºæ¨¡æ‰ç®—å®Œæˆ</p>

<p>å¯å¾®ç¥ç»ç½‘ç»œï¼Œæˆ–è€…è¯´ç¥ç»ç½‘ç»œï¼Œæ˜¯å¸¸ç”¨çš„è®¡ç®—å¯è¡Œçš„ï¼Œæ³›åŒ–æ€§å¥½çš„æ–¹æ¡ˆï¼Œä½œä¸ºå‡½æ•°æ‹Ÿåˆå™¨ï¼ŒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œä¸€æ ·èƒ½åšåˆ°å¯ä»¥å­¦ä¹ æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚å› æ­¤åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¦‚ç‡æ¨¡å‹ï¼Œç”±äºå…¶è®¡ç®—å¯è¡Œæ€§ï¼Œä»¥åŠå¯¹äºç¥ç»ç½‘ç»œæœ‰ç›¸å½“å¥½çš„ä¼˜åŒ–æ–¹æ¡ˆå¦‚éšæœºæ¢¯åº¦ä¸‹é™ï¼Œå› æ­¤ä½œä¸ºæ–‡ä¸­çš„å…·ä½“æ¨¡å‹ï¼Œå†™ä½œ \(NeuralNet(.)\)</p>

<p>æ¯”å¦‚åˆ©ç”¨ç¥ç»ç½‘ç»œå»å»ºæ¨¡ä¸€ä¸ªå›¾ç‰‡åˆ†ç±»æ¨¡å‹ \(p_{\boldsymbol{\theta}}(y \mid \mathbf{x})\) ï¼Œ\(y\) ä»£è¡¨ç±»ï¼Œ \(\mathbf{x}\) ä»£è¡¨å›¾ç‰‡ï¼š</p>

\[\begin{aligned}
\mathbf{p} &amp;=\text { NeuralNet }(\mathbf{x}) \\
p_{\boldsymbol{\theta}}(y \mid \mathbf{x}) &amp;=\text { Categorical }(y ; \mathbf{p})
\end{aligned}\]

<p>ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†ä½¿å¾—è¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œä¼šåˆ©ç”¨å¦‚ softmax å±‚ç­‰ä½œä¸º \(NeuralNet(.)\) çš„æœ€åä¸€å±‚æ¥è§„èŒƒè¾“å‡ºä½¿ \(\sum_{i} p_{i}=1\)</p>

<h3 id="13--directed-graphical-models-and-neural-networks">1.3  Directed Graphical Models and Neural Networks</h3>

<p>åˆ°å…·ä½“çš„æ¦‚ç‡æ¨¡å‹å»ºæ¨¡æ—¶ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾å»ºç«‹æ¦‚ç‡å˜é‡ä¹‹é—´çš„è”ç³»ï¼Œç§°ä¸ºæ¦‚ç‡å›¾æ¨¡å‹ï¼Œæ¯”å¦‚ä¸‹å›¾è¿™æ ·çš„ï¼š</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/440px-Graph_model.svg.png" alt="440px-Graph_model.svg" style="zoom:50%;" /></p>

<p>äºæ˜¯åœ¨æœ‰å‘æ— ç¯å›¾å»ºæ¨¡ä¸‹çš„æ¦‚ç‡æ¨¡å‹ï¼Œå…¶è”åˆæ¦‚ç‡å¯ä»¥å†™æˆï¼š</p>

\[p_{\boldsymbol{\theta}}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{M}\right)=\prod_{j=1}^{M} p_{\boldsymbol{\theta}}\left(\mathbf{x}_{j} \mid P a\left(\mathbf{x}_{j}\right)\right)\]

<p>\(P a\left(\mathbf{x}_{j}\right)\) æŒ‡ä»£çš„æ˜¯èŠ‚ç‚¹ \(j\) çš„æ‰€æœ‰çˆ¶èŠ‚ç‚¹ï¼Œè€Œå¯¹äºæ ¹èŠ‚ç‚¹ï¼Œå®šä¹‰å…¶ \(P a\left(\mathbf{x}_{j}\right)\) ä¸ºç©ºé›†</p>

<p>ä¸ºäº†å…·ä½“çš„å‚æ•°åŒ–æœ‰å‘æ— ç¯å›¾æ¦‚ç‡æ¨¡å‹ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡ï¼Œç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯ä¸€ä¸ªéšæœºå˜é‡ \(\mathbf{x}\) çš„çˆ¶èŠ‚ç‚¹  \(P a(\mathbf{x})\)ï¼Œè¾“å‡ºçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒ \(\eta\) ï¼š</p>

\[\begin{aligned}
\boldsymbol{\eta} &amp;=\operatorname{Neural} \operatorname{Net}(P a(\mathbf{x})) \\
p_{\boldsymbol{\theta}}(\mathbf{x} \mid P a(\mathbf{x})) &amp;=p_{\boldsymbol{\theta}}(\mathbf{x} \mid \boldsymbol{\eta})
\end{aligned}\]

<p>ä¸‹é¢å™è¿°å¦‚ä½•å­¦ä¹ è¿™æ ·çš„æ¨¡å‹çš„å‚æ•°</p>

<h3 id="14--learning-in-fully-observed-models-with-neural-nets">1.4  Learning in Fully Observed Models with Neural Nets</h3>

<p>åœ¨æœ‰å‘æ— ç¯å›¾æ¦‚ç‡æ¨¡å‹ä¸­ï¼Œå¦‚æœåœ¨å›¾ä¸­æ‰€æœ‰çš„éšæœºå˜é‡éƒ½åœ¨æ•°æ®ä¸­è¢«è§‚å¯Ÿåˆ°äº†ï¼Œé‚£ä¹ˆéœ€è¦åšåˆ°å°±æ˜¯å¸¸è§„çš„ MLE ä¼˜åŒ–ï¼Œè®¡ç®—ï¼Œå¾®åˆ†ï¼ˆStraightforwardï¼ï¼‰</p>

<h4 id="141-dataset">1.4.1 Dataset</h4>

<p>å¯¹é‡‡æ ·è¿‡ç¨‹æœ‰ \(i.i.d\) å‡è®¾</p>

\[\mathcal{D}=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)}\right\} \equiv\left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N} \equiv \mathbf{x}^{(1: N)}\]

<p>å› æ­¤ï¼Œåˆ©ç”¨å–logåˆ†è§£è¿ä¹˜æœ‰ï¼š</p>

\[\log p_{\boldsymbol{\theta}}(\mathcal{D})=\sum_{\mathbf{x} \in \mathcal{D}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\]

<h4 id="142-maximum-likelihood-and-minibatch-sgd">1.4.2 Maximum Likelihood and Minibatch SGD</h4>

<p>åœ¨ ML æ ‡å‡†ä¸­ï¼Œä¼˜åŒ–åœ¨ç»™å‡ºæ ‡å‡†åï¼Œå–å¯»æ‰¾æœ€ä¼˜å‚æ•° \(\theta^*\) ä½¿å¾—æ ‡å‡†æœ€ä¼˜ï¼Œæ¯”å¦‚</p>

\[\theta^* = \arg\min_{\theta} \sum_{\mathbf{x} \in \mathcal{D}} \log p_{\boldsymbol{\theta}}(\mathbf{x})\]

<p>è€Œå¸¸ç”¨çš„æ±‚è§£ç®—æ³•æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆSGDï¼‰ï¼Œå½“åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œä¸€æ¬¡æ¢¯åº¦è®¡ç®—çš„è¯ï¼Œ ç§°ä¸º batch gradient descentï¼Œä½†æ˜¯å½“æ•°æ®é›†å¾ˆå¤§çš„æ—¶å€™ï¼Œæ›´åŠ é€‚åˆç”¨ minibatch SGD æ¥å¤„ç†</p>

<h3 id="15-intractabilities">1.5 Intractabilities</h3>

<p>DLVMï¼Œæ·±åº¦éšå˜é‡æ¨¡å‹ï¼Œå»ºç«‹åœ¨éšç©ºé—´ä¸Šï¼Œéšç©ºé—´ä¸­çš„å˜é‡æ˜¯éš¾ä»¥ç›´æ¥è§‚å¯Ÿå¾—åˆ°çš„ï¼Œå¾€å¾€å¾ˆéš¾åœ¨æ•°æ®ä¸­ç›´æ¥ä½“ç°ï¼Œä½†æ˜¯ç¡®å®å­˜åœ¨ç€éšç©ºé—´ï¼Œä»¥mnistæ‰‹å†™ä½“æ•°æ®åº“æ¥è¯´çš„è¯ï¼Œå°½ç®¡æ‰‹å†™ä½“çš„ç»´åº¦æ—¶ \(28 \times 28\) ç»´ï¼Œä½†æ˜¯å®é™…ä¸Šåœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œåœ¨æ‰‹å†™è¿‡ç¨‹ä¸­ï¼Œå¤§è„‘å»æ„å»ºæ‰‹å†™ä½“æ•°å­—çš„ç»“æ„æ—¶ï¼Œæƒ³çš„æ˜¯ç¬”ç”»å¤šé‡ï¼Œå¼¯æŠ˜æ€ä¹ˆå†™ï¼Œè¿™éƒ¨åˆ†æ€æƒ³è¿‡ç¨‹å¯ä»¥å½’çº³åˆ°éšç©ºé—´å»ºæ¨¡ä¸Š</p>

<p>ç”Ÿæˆæ¨¡å‹ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œç›´æ¥å»ºç«‹ \(p(\mathbf{x})\) å¾ˆéš¾ï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ä¸€ç»„éšå˜é‡æ¥å»ºæ¨¡ï¼š</p>

\[p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z} = \int p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) d \mathbf{z}\]

<p>ä½†æ˜¯åœ¨å®é™…å»ºæ¨¡ä¸­ï¼Œä¸Šè¿°çš„æ¨¡å‹æ—¶å¾ˆéš¾è®¡ç®—çš„ï¼Œå³ DLVM çš„ä¸å¯å¤„ç†æ€§</p>

<p>æœ€å¤§ä¼¼ç„¶ä¼°è®¡åœ¨ DLVM éœ€è¦è§£æçš„ï¼Œæˆ–è€…ä¼°è®¡å‡º \(p_{\boldsymbol{\theta}}(\mathbf{x})\)ï¼Œå†é€šè¿‡ä¼˜åŒ– \(p_{\boldsymbol{\theta}}(\mathbf{x})\) æ‰¾å‡º \(\theta^*\)ï¼Œä½†æ˜¯ç”±äºæ±‚è§£ç§¯åˆ† \(p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z}\) çš„è¿‡ç¨‹ï¼Œè¿™ä¸ªä¸èƒ½ç›´æ¥è§£æçš„åšå‡ºï¼Œæˆ–ä¼°è®¡å‡ºï¼Œé€šè¿‡ MLE ä¼°è®¡æ¥ä¼˜åŒ–æ˜¯ä¸å¯å¤„ç†çš„ï¼ŒåŒæ—¶è¿å¸¦çš„ï¼Œç”±äºåéªŒåˆ†å¸ƒæœ‰è®¡ç®—å…¬å¼ï¼š</p>

\[p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})=\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{x})}\]

<p>å› æ­¤åéªŒåˆ†å¸ƒä¹Ÿæ˜¯ä¸èƒ½è®¡ç®—çš„ï¼Œå¦‚ MAP ç­‰æ–¹æ³•åŒæ ·ä¸èƒ½ä½¿ç”¨ï¼Œä¸ºäº†è§£å†³ä¸å¯å¤„ç†æ€§ï¼Œç”Ÿæˆæ¨¡å‹ä¸­æœ‰å‡ ç±»æ–¹æ¡ˆï¼Œä¸‹é¢ä¸»è¦å…³æ³¨ Variational Autoencoders</p>

<h2 id="2-variational-autoencoders">2. Variational Autoencoders</h2>

<p>VAE ä»æœ€å¤§è¾¹é™…åéªŒ \(p_{\theta}(\mathbf{x})\) å‡ºå‘ï¼Œæƒ³æ³•æœ‰å€Ÿé‰´è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAutoencoderï¼‰ï¼Œä½†æ˜¯ç”±äºè¾¹é™…åéªŒä¸å¯å¤„ç†æ€§ï¼ŒVAE çš„æ€è·¯æ˜¯é€šè¿‡åšä¸€ä¸ªå‚æ•°æ¨æ–­æ¨¡å‹ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) æ¥æ›¿æ¢ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) ï¼Œå…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ç”¨ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) é€¼è¿‘ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) ï¼š</p>

\[q_{\phi}(\mathbf{z} \mid \mathbf{x}) \approx p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\]

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/VAE-illustration.png" alt="VAE-illustration" style="zoom:30%;" /></p>

<p>æ­£å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¦‚æœå¼ºè°ƒ VAE çš„è‡ªåŠ¨ç¼–ç å™¨çš„å±æ€§ï¼Œåˆ™æ¨æ–­åˆ†å¸ƒ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) æ„æˆäº†ç¼–ç å™¨ï¼Œè§£ç å™¨åˆ™æ˜¯ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)ï¼Œå½“ç„¶å¦‚æœèƒ½å¤Ÿç›´æ¥æ±‚è§£ \(p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\) æ˜¯æœ€å¥½çš„ï¼Œä½†æ˜¯ä¸Šè¿°è§£ç å™¨æ˜¯æ— æ³•ç›´æ¥ä¼˜åŒ–çš„ï¼Œå› æ­¤æ‰éœ€è¦å†åšä¸€ä¸ªç¼–ç å™¨çš„æ¨æ–­åˆ†å¸ƒ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\)</p>

<p>åœ¨å…·ä½“é—®é¢˜ä¸­ï¼Œé€šè¿‡ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ç»“æ„æ¥å»ºæ¨¡æ¨æ–­åˆ†å¸ƒï¼š</p>

\[q_{\phi}(\mathbf{z} \mid \mathbf{x})=q_{\phi}\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{M} \mid \mathbf{x}\right)=\prod_{j=1}^{M} q_{\phi}\left(\mathbf{z}_{j} \mid P a\left(\mathbf{z}_{j}\right), \mathbf{x}\right)\]

<p>ä¸ºäº†å…·ä½“çš„å»ºæ¨¡è¿™ä¸ªé€¼è¿‘ï¼ŒVAE é‡‡ç”¨äº†ä¸¤æ­¥ï¼Œå³ \(Evidence \; Lower \; Bound\) æ¥ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼ŒåŒæ—¶åœ¨å…·ä½“ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡‡ç”¨äº† \(Reparameterization\; Trick\) æ¥è§£å†³æ¢¯åº¦çš„æ±‚è§£é—®é¢˜</p>

<h3 id="21-loss-function-evidence-lower-bound">2.1 Loss function (Evidence Lower Bound)</h3>

<p>è¾¹é™…åéªŒ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) ä¸ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ç‹¬ç«‹ï¼Œå› æ­¤é€šè¿‡é‡å†™æˆæœŸæœ›ï¼Œå¯ä»¥æ¨å¯¼å‡º \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) ç­‰ä»·äº ELBO åŠ ä¸€ä¸ªKLæ•£åº¦</p>

\[\begin{aligned}
\log p_{\boldsymbol{\theta}}(\mathbf{x}) 
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x})\right] 
\\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;= \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]
\\
&amp;= \underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\\(\mathrm{ELBO})}
+
\underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=D_{K L}\left(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)}

\end{aligned}\]

<p>é‚£ä¹ˆæœ€å¤§åéªŒè¿‘ä¼¼äºä¼˜åŒ– \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) (ELBO)</p>

\[\begin{aligned}
\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x}) 
&amp;= \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]
\\
 &amp;=\log p_{\boldsymbol{\theta}}(\mathbf{x})-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right) \\
&amp; \leq \log p_{\boldsymbol{\theta}}(\mathbf{x})
\end{aligned}\]

<p>è€Œç¬¬äºŒé¡¹KLæ•£åº¦ï¼Œå¾ˆå·§å¦™çš„åŒæ—¶è¡¨è¾¾äº†ä¸¤ä¸ªè·ç¦»ï¼š</p>

<ol>
  <li>ELBOå’Œè¾¹é™…åéªŒä¸­é—´çš„è·ç¦»</li>
  <li>æ¨æ–­åˆ†å¸ƒ \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ä¸åéªŒ \(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\) çš„è·ç¦»</li>
</ol>

<p>ä¼˜åŒ–ç›®æ ‡ä»è¾¹é™…åéªŒåˆ°  \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) ï¼Œæ¯”èµ·ä¸å¯å¤„ç†çš„è¾¹é™…åéªŒï¼Œ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) èƒ½å¤Ÿé€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ï¼Œç†è®ºä¸Šæ¥è¯´ä¼˜åŒ– \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) å¯ä»¥åŒæ—¶è¾¾æˆä¸¤ä¸ªç›®æ ‡ï¼š</p>

<ol>
  <li>ä¼˜åŒ–è¾¹é™…åˆ†å¸ƒ \(\log p_{\boldsymbol{\theta}}(\mathbf{x})\) çš„å˜åˆ†ä¸‹ç•Œï¼Œä½¿å¾—è¾¹é™…åˆ†å¸ƒä¸å°ï¼Œè¿‘ä¼¼çš„è¾¾æˆæœ€å¤§ä¼¼ç„¶çš„æ•ˆæœ</li>
  <li>ä½¿ \(\mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) ç¬¬äºŒé¡¹ \(-D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)\) å˜å¤§ï¼Œå³ä½¿æ¨æ–­åˆ†å¸ƒå’ŒåéªŒåˆ†å¸ƒçš„è·ç¦»å˜å°ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´å‡†ç¡®çš„æ¨æ–­åˆ†å¸ƒ</li>
</ol>

<h3 id="22-stochastic-gradient-based-optimization-of-the-elbo-reparameterization-trick">2.2 Stochastic Gradient-Based Optimization of the ELBO (Reparameterization Trick)</h3>

<h4 id="221-reparameterization-trick">2.2.1 Reparameterization Trick</h4>

<p>ç»™ä¸€ä¸ª \(i.i.d\) æ•°æ®é›† \(\mathcal{D}\)ï¼Œæ•°æ®é›†äº§ç”Ÿ \(\mathcal{L}_{\theta, \phi}(\mathcal{D})\) ç­‰ä»·äº \(\sum_{\mathbf{x} \in \mathcal{D}} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)ï¼Œé‚£ä¹ˆä¸‹é¢çš„è®¨è®ºä¸­ï¼Œå°±å˜æˆè®¡ç®— \(\mathcal{L}_{\theta, \phi}(\mathbf{x})\)ï¼Œè¿™æ ·æ˜¯ç­‰ä»·çš„ï¼Œé‡‡ç”¨æ¢¯åº¦æ³•å»ä¼˜åŒ– \(\sum_{\mathbf{x} \in \mathcal{D}} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)</p>

<p>æ¢¯åº¦ \(\nabla_{\boldsymbol{\theta}, \phi} \mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})\) ï¼Œå¯¹äº \(\theta\) æœ‰ï¼š</p>

\[\begin{aligned}
\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) &amp;=\nabla_{\boldsymbol{\theta}} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right] \\

&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right)\right] \\

&amp; = \nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right) \\

&amp;=\nabla_{\boldsymbol{\theta}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})\right)
\end{aligned}\]

<p>ä½†æ˜¯é—®é¢˜åœ¨äºï¼Œå¯¹äº \(\phi\)ï¼Œæ¢¯åº¦éš¾ä»¥ç›´æ¥è®¡ç®—</p>

\[\begin{aligned}
\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x}) &amp;=\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right] \\
&amp; \neq \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\nabla_{\phi}\left(\log p_{\theta}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\right)\right]
\end{aligned}\]

<p>VAE æä¾›çš„è§£å†³åŠæ³•æ˜¯å»æ„é€ ä¸€ä¸ª \(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\) æ— åä¼°è®¡ \(\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\) ï¼Œå³å¦ä¸€ä¸ªæŠ€å·§ \(Reparameterization \; Trick\)</p>

<p>\(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\) æ˜¯ç”±äºè®¡ç®— \(q_{\phi}(\mathbf{z} \mid \mathbf{x})\) çš„æœŸæœ›ï¼Œå†æ­¤åŸºç¡€ä¸Šç®— \(\phi\) çš„åå¯¼æ•°å°±å¯¼è‡´éš¾ä»¥è§£æçš„è®¡ç®—ï¼Œ\(Reparameterization \; Trick\) è®¾è®¡äº†ä¸€ä¸ªæ— åç»Ÿè®¡é‡æ¥ä¼°è®¡ \(\nabla_{\phi} \mathcal{L}_{\theta, \phi}(\mathbf{x})\)</p>

<p style="text-align: center;"><img src="/images/2021-01-25-Generative Model Part 2ï¼šGenerative Model Part 2ï¼šA Survey on Variational Autoencoders.md/Reparameterization-Trick.png" alt="Reparameterization-Trick" style="zoom:40%;" /></p>

<p>å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ„é€ ä¸€ä¸ªå™ªå£°å˜é‡ \(\epsilon\) æœ‰åˆ†å¸ƒ \(p(\boldsymbol{\epsilon})\)ï¼Œå®šä¹‰æ–°çš„æ˜ å°„å…³ç³»ï¼š\(\mathbf{z}=\mathbf{g}(\boldsymbol{\epsilon}, \boldsymbol{\phi}, \mathbf{x})\) æ»¡è¶³</p>

\[\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[f(\mathbf{z})]=\mathbb{E}_{p(\boldsymbol{\epsilon})}[f(\mathbf{z})]\]

<p>åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ±‚å¯¼å°±å¯ä»¥æœ‰ä¸€ä¸ª \(\nabla_{\boldsymbol{\phi}} f(\mathbf{z})\) çš„ä¼°è®¡ï¼š</p>

\[\begin{aligned}
\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[f(\mathbf{z})] &amp;=\nabla_{\phi} \mathbb{E}_{p(\boldsymbol{\epsilon})}[f(\mathbf{z})] \\
&amp;=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\phi}} f(\mathbf{z})\right] \\
&amp; \simeq \nabla_{\boldsymbol{\phi}} f(\mathbf{z})
\end{aligned}\]

<p>æ›´è¿›ä¸€æ­¥ï¼Œä»£å…¥å…·ä½“çš„å‡½æ•° \(f(z) = \log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\)ï¼Œæ„é€ ä¸€ä¸ªè’™ç‰¹å¡æ´›ä¼°è®¡
\(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x})\)ï¼š</p>

\[\begin{aligned}
\boldsymbol{\epsilon} &amp; \sim p(\boldsymbol{\epsilon})
\\
\mathbf{z} &amp;=\mathbf{g}(\boldsymbol{\phi}, \mathbf{x}, \boldsymbol{\epsilon}) 
\\
\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) &amp;=\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})
\end{aligned}\]

<p>å¯ä»¥è¯æ˜ \(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x})\) æ˜¯æ— åçš„ï¼š</p>

\[\begin{aligned}
\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\theta}, \phi} \tilde{\mathcal{L}}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x} ; \boldsymbol{\epsilon})\right] &amp;=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right)\right] \\
&amp;=\nabla_{\boldsymbol{\theta}, \boldsymbol{\phi}}\left(\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]\right) \\
&amp;=\nabla_{\boldsymbol{\theta}, \phi} \mathcal{L}_{\boldsymbol{\theta}, \boldsymbol{\phi}}(\mathbf{x})
\end{aligned}\]

<h4 id="222-computation-of-inference-distribution">2.2.2 Computation of Inference Distribution</h4>

<p>åœ¨è®¡ç®— \(\tilde{\mathcal{L}}_{\boldsymbol{\theta}, \phi}(\mathbf{x}) =\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})-\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\) çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦è®¡ç®—ç¼–ç å™¨åˆ†å¸ƒ \(\log q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\)</p>

\[\begin{aligned}
\boldsymbol{\epsilon} &amp; \sim p(\boldsymbol{\epsilon})
\\
\mathbf{z} &amp;=\mathbf{g}(\boldsymbol{\phi}, \mathbf{x}, \boldsymbol{\epsilon}) 
\\
p(\boldsymbol{\epsilon}) &amp;= q_{\phi}(\mathbf{z} \mid \mathbf{x})\left|\operatorname{det}\left(\frac{\partial \mathbf{z}}{\partial \boldsymbol{\epsilon}}\right)\right|

\end{aligned}\]

<p>å› æ­¤å¯ä»¥çœ‹å‡ºï¼Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å¯é€†å˜åŒ– \(g(\epsilon)\) ä¼šä½¿å¾—è®¡ç®—ç®€åŒ–</p>

\[\log q_{\phi}(\mathbf{z} \mid \mathbf{x})=\log p(\boldsymbol{\epsilon})-\log \left|\operatorname{det}\left(\frac{\partial \mathbf{z}}{\partial \boldsymbol{\epsilon}}\right)\right|\]

<p>ä¸‹é¢ä¸¾ä¸€ä¸ªå…·ä½“è®¡ç®—ä¸­æ„é€ æ¨æ–­åˆ†å¸ƒå’Œ \(Reparameterization \; Trick\) çš„ä¾‹å­ï¼š</p>

<p>æ¨æ–­åˆ†å¸ƒï¼ˆç¼–ç å™¨åˆ†å¸ƒï¼‰å–ä½œé«˜æ–¯åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°é€šè¿‡ç¼–ç å™¨è®¡ç®—å¾—åˆ°ï¼š</p>

\[\begin{aligned}
q_{\phi}(\mathbf{z} \mid \mathbf{x})  &amp;= \mathcal{N}(\mathbf{z} ; \boldsymbol{\mu}, \operatorname{diag}\left(\sigma^{2}\right))
\\
(\boldsymbol{\mu}, \log \boldsymbol{\sigma}) &amp;=\text { EncoderNeuralNet}_{\boldsymbol{\phi}}(\mathbf{x}) 
\\
q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) &amp;=\prod_{i} q_{\boldsymbol{\phi}}\left(z_{i} \mid \mathbf{x}\right)=\prod_{i} \mathcal{N}\left(z_{i} ; \mu_{i}, \sigma_{i}^{2}\right)
\end{aligned}\]

<p>é‡‡ç”¨ \(Reparameterization \; Trick\)ï¼š</p>

\[\begin{array}{l}
\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}) \\
\mathbf{z}=\mu+\sigma \odot \epsilon
\end{array}\]

<p>å› æ­¤å°±å¯ä»¥è®¡ç®—åéªŒåˆ†å¸ƒ \(\log q_{\phi}(\mathbf{z} \mid \mathbf{x})\) ä¸ºï¼š</p>

\[\begin{aligned}
\log q_{\phi}(\mathbf{z} \mid \mathbf{x}) &amp;=\log p(\boldsymbol{\epsilon})-\log d_{\boldsymbol{\phi}}(\mathbf{x}, \boldsymbol{\epsilon}) \\
&amp;=\sum_{i} \log \mathcal{N}\left(\epsilon_{i} ; 0,1\right)-\log \sigma_{i}
\end{aligned}\]

<h3 id="23-estimation-of-the-marginal-likelihood">2.3 Estimation of the Marginal Likelihood</h3>

<p>è¾¹é™…ä¼¼ç„¶å¯ä»¥åˆ©ç”¨æ¨æ–­åˆ†å¸ƒå»æ„é€ ï¼š</p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x})=\log \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) / q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})\right]\]

<p>è€Œåœ¨æ•°å€¼ä¸Šï¼Œä¸Šå¼å¯ä»¥åˆ©ç”¨è’™ç‰¹å¡æ´›æ³•è¿›è¡Œä¼°è®¡ï¼š</p>

\[\log p_{\boldsymbol{\theta}}(\mathbf{x}) \approx \log \frac{1}{L} \sum_{l=1}^{L} p_{\boldsymbol{\theta}}\left(\mathbf{x}, \mathbf{z}^{(l)}\right) / q_{\boldsymbol{\phi}}\left(\mathbf{z}^{(l)} \mid \mathbf{x}\right)\]

<p>å½“ \(L=1\) æ—¶ï¼Œè¿™ä¸ªå¼å­å°±æ˜¯ \(\mathcal{L}_{\theta, \phi}(\mathcal{x})\)</p>

<h3 id="24-challenges">2.4 Challenges</h3>

<p>VAEï¼Œå’ŒGANä¸€æ ·ï¼Œæœ€åé€šè¿‡ä»éšç©ºé—´é‡‡æ ·ï¼Œç„¶åç”¨è§£ç å™¨æ˜ å°„åˆ°æ•°æ®ç©ºé—´ï¼Œç”Ÿæˆæ•°æ®ï¼Œä½†æ˜¯VAEä»ç„¶ä¼šå»è®­ç»ƒ</p>

<h4 id="241-optimization-issues">2.4.1 Optimization issues</h4>

<h3 id="25-summary-of-training--generation">2.5 Summary of Training &amp; Generation</h3>

<p>VAEï¼Œå’ŒGANä¸€æ ·ï¼Œæœ€åé€šè¿‡ä»éšç©ºé—´é‡‡æ ·ï¼Œç„¶åç”¨è§£ç å™¨æ˜ å°„åˆ°æ•°æ®ç©ºé—´ï¼Œç”Ÿæˆæ•°æ®ï¼Œä½†æ˜¯æ¯”èµ·GANé€šè¿‡å»è¿­ä»£è®­ç»ƒç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„ç»“æ„ï¼Œæœ€åæŠŠç”Ÿæˆå™¨å½“ä½œæ˜ å°„ï¼ŒVAEæ›´åŠ å¼ºè°ƒå¯¹äºæ•°æ®åˆ†å¸ƒçš„ç†è§£</p>

:ET