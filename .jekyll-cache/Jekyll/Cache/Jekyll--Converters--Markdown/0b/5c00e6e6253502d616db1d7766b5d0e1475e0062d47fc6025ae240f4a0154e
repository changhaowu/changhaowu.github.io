I"U<ul class="table-of-content" id="markdown-toc">
  <li><a href="#analytical-function-case" id="markdown-toc-analytical-function-case">Analytical Function Case</a></li>
  <li><a href="#intermediated-states-case" id="markdown-toc-intermediated-states-case">Intermediated States Case</a></li>
  <li><a href="#ode-controlled-case" id="markdown-toc-ode-controlled-case">ODE controlled Case</a></li>
</ul>
<p>Adjoint States Methods 在数值计算中非常的有用，其在1960年代由 Pontryagin 提出以更高效的计算梯度，在如地震学，冰川学等学科的数值模拟中广泛应用，而近来神经网络的优化问题范式同样符合 Adjoint States Methods 使用范畴，当在神经网络中使用时，他还有个更有名的名字 ‘Back Propagation’，更巧妙的是，在 NIPS 2018 best paper winner ‘Neural Ordinary Differential Equations’ 中，Adjoint States Methods 的应用使得梯度对于内存的计算复杂度从 O(n) 降至 O(1)，换言之，从要把整个网络的计算图存下来的一般 Back Propagation, Adjoint States Methods 把梯度下降变成了一个逐层迭代求解的问题</p>

<p>于是现在优化问题的定义开始，现在有输入 $x \in \mathbb{R}^{n_{x}}$，控制 $p \in \mathbb{R}^{n_{p}}$, 有优化问题：</p>

\[\arg_{p} \min J(x, p) \ \ \ J: \mathbb{R}^{n_{x}} \times \mathbb{R}^{n_{p}} \rightarrow \mathbb{R}
\\
s.t \ \ \ f(x, p)=0  \ \ \ f: \mathbb{R}^{n_{x}} \times \mathbb{R}^{n_{p}} \rightarrow \mathbb{R}^{n_{x}}\]

<p>如果以神经网络的视角来看的话，$J$ 即代表损失函数，而控制 $p$ 其实就是控制网络的参数，逐层来定义网络中的 hidden layers</p>

<p>为了对研究的情况做一些分类， 可以从 $J(x, p)$ 的角度来看，在 ‘16-90-computational-methods-in-aerospace-engineering’ 中， Qiqi Wang 教授定义了五种情况：</p>

<ol>
  <li>
    <p>解析的 $J(x, p)$，这样直接计算即可</p>
  </li>
  <li>
    <p>存在 intermediate state 的 $x_i$ 的 $J(x_i, p)$​, 神经网络就属于这种情况</p>

\[x_{n}=x_{n}\left(x_{n-1}, p\right)
\\
x_{0}=x_{0}(p)\]
  </li>
  <li>
    <p>$x$ 由一个ODE控制</p>

\[J(x(t), p)   \ \ \    s.t \ \frac{d x}{d t}=f(x, p)\]
  </li>
  <li>
    <p>$x,p$ 由一个隐函数 $f(x, p)=0$ 控制</p>
  </li>
  <li>
    <p>$x$ 由一个PDE控制, $J(x(y,t), p) $</p>
  </li>
</ol>

<p>下文的行文结构做如此安排：通过第一种情况来叙述 Adjoint States Method 的思想，通过第二种情况来展现 Back Propagation 和 Adjoint States Method 的等价性，通过第三种情况来分析 Neural ODEs 中利用 Adjoint States Method 来减少计算内存复杂度的方法</p>

<h3 id="analytical-function-case">Analytical Function Case</h3>

<p>通过定义拉格朗日函数 $\mathcal{L}(x, p, \lambda) \equiv J(x,p)+\lambda^{T} f(x, p)$， 同时由于 $f(x, p) = 0$, 可知 $x = x(p)$，这样可以化代价函数 $J(x,p)$ 为 $J(x(p))$</p>

\[\begin{aligned} 
\mathrm{d}_{p} f(x)=\mathrm{d}_{p} \mathcal{L} &amp;=\partial_{x} J \mathrm{~d}_{p} x+\mathrm{d}_{p} \lambda^{T} f+\lambda^{T}\left(\partial_{x} f \mathrm{~d}_{p} x+\partial_{p} f\right) 
\\
&amp;=J_{x} x_{p}+ \lambda^{T}\left(f_{x} x_{p}+f_{p}\right) \quad \text { because } f=0 \text { everywhere }
\\ 
&amp;=\left(J_{x}+\lambda^{T} f_{x}\right) x_{p}+\lambda^{T} f_{p}  
\end{aligned}\]

<p>为了方便计算 $d_p f $，其中 $f_{p},J_{p}$ 都是可以直接解析计算的，而 $x_{p}$ 则需要通过整个网络迭代重新计算，为了计算梯度的便捷性，得到了拉格朗日乘子 $f_{x}^{T} \lambda=-J_{x}^{T}$，那么 $d_p J=\lambda^{T} f_p $</p>

<p>而 $\lambda = -(f_{x}^{-1})^{T}J_{x}^{T}$ ，这种共轭转置被称为 adjoint variable，这也是这样计算梯度的方法被称为 adjoint state method 的原因</p>

<h3 id="intermediated-states-case">Intermediated States Case</h3>

<p>现在有如下计算图:</p>

<p style="text-align: center;"><img src="/images/2021-12-07-Adjoint-Methods-and-Auto-Differentiation/intermediate_states_graph.png" alt="intermediate_states_graph" style="zoom:40%;" /></p>

<h3 id="ode-controlled-case">ODE controlled Case</h3>

:ET